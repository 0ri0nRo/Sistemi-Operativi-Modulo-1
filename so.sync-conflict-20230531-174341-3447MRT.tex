\documentclass{article}
\usepackage{graphicx}
\usepackage[a4paper, left=2cm, right=2cm, top=2.5cm, bottom=2cm]{geometry}
\usepackage[small]{titlesec}
\usepackage{subcaption} 
\usepackage[italian]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{sectsty}
\usepackage{listings}
%\usepackage[light]{CormorantGaramond}
\usepackage{fancyhdr}
\usepackage{footnote}
\usepackage{tgadventor}
\usepackage{titling}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}

\setlength{\droptitle}{-5em}   % regola la posizione del titolo
\pretitle{\begin{center}\LARGE}   % imposta la dimensione della font del titolo
\posttitle{\end{center}}
%\lstset{language=C}
\renewcommand\bfdefault{bx}
\sectionfont{\fontsize{20.74}{15}\selectfont}
\subsectionfont{\fontsize{17.28}{15}\selectfont}
\subsubsectionfont{\fontsize{12}{15}\selectfont}
\title{}

\author{}
\date{\today}


\begin{document}
\pagestyle{fancy}
\fancyhf{}
%\rhead{\thepage}
%\lhead{\rightmark}
\rfoot{\thepage}
\lhead{\quad \leftmark}
\rhead{ \quad \rightmark}

\renewcommand{\headrulewidth}{0.2pt}



\begin{titlepage}
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.7\textwidth]{im/logo_sapienza_new.png}
        \label{fig:logo}
    \end{figure}    
    \null\vfill
    \begin{center}
      {\Huge Appunti di Sistemi Operativi I} \\[2cm]
      {\Large Colacel Alexandru Andrei}
    \end{center}
    \vfill\null
    \renewcommand{\abstractname}{Disclaimer}

    
    
    \begin{abstract}  
    
    \hrulefill


    Le fonti sono le slides del Prof. Tolomei, appunti di colleghi presi a lezione e integrazioni con il libro \mbox{"\textit{I moderni sistemi operativi IV edizione}"} di Andrew S. Tanenbaum. Se diversamente verrà indicato a pié di pagina.\\
    \textbf{Nota: è vietata assolutamente la vendita di questo materiale in qualsiasi forma senza il mio consenso.}  

    
    \hrulefill

    \end{abstract}
  \end{titlepage}


\pagebreak
\tableofcontents

\pagebreak


\section{Introduzione}
Il programma con cui si interagisce può essere formato testo (quindi la \textbf{\$SHELL}) oppure modalità \textbf{GUI} (Graphical User Interface) quando ci sono le icone. La maggior parte dei computer ha due modalità operative: kernel e utente. Il sistema operativo è il componente software di maggior importanza e viene eseguito in modalità kernel (detta anche Supervisor). In questo modo ha accesso a tutto l'hardware e può eseguire qualunque istruzione che la macchina sia in  grado di svolgere. La modalità utente ha a sua disposizione solo un sottoinsieme di istruzioni.\\
Esiste una differenza tra il sistema operativo (che troverete scritto anche OS) e il normale software in modalità utente. Infatti l'utente non è livero di scrivere E.g un gestore degli interrupt ecc\dots perchè esistono protezioni hardware dell'OS stesso. \\
Questa differenza è meno evidente nei sistemi embedded o sistemi integrati (che possono non avere la modalità kernel) o sistemi interpretati che usano interpreti e non hardware per separare i componenti.

\subsection{Kernel/User mode e protezione della memoria}
Alcune istruzioni eseguite dalla CPU risultano essere più sensibili di altre. Affinché tali istruzioni privilegiate vengano utilizzate esclusivamente dal sistema operativo, la CPU può essere impostata in due modalità specifiche a seconda del programma in esecuzione.
La CPU può essere quindi impostata in:
\begin{itemize}
    \item Kernel mode ossia in modalità senza alcuna restrizione, permettendo l'esecuzione di qualsiasi istruzione (utilizzata dal sistema operativo)
    \item User mode, dove non sono possibili:
    \begin{itemize}
        \item Accedere agli indirizzi riservati ai dispositivi di I/O 
        \item Manipolare il contenuto della memoria principale
        \item Arrestare il sistema
        \item Passare alla Kernel Mode
        \item \dots
    \end{itemize}

\end{itemize}
Per poter impostare una delle due modalità, viene utilizzato un bit speciale salvato in un registro protetto: se impostato su 0 la CPU sarà in Kernel mode, mentre se impostato su 1 la CPU sarà in User mode
\subsection{Che cos'è un Sistema Operativo}

Il Sistema Operativo esegue fondamentalmente due funzioni non correlate. Da una parte fornisce ai programmatori funzioni di applicazioni un insieme di risorse astratte e dall'altra le risorse hardware.
\subsubsection{L'OS come macchina estesa}

L'archiettura è l'insieme delle istruzioni, organizzazione della memoria, I/0, e struttura dei bus. Per la gestione dell'hardware si utilizza un software chiamato \textbf{driver} che fornise l'interfaccia per la lettura e la scrittura senza che il programmatore si occupi dei dettagli.
L'astrazione è la chiave per risolvere la complessità, una buona astrazione suddivide un'attività complessa in due attività più gestibili. La prima riguarda la definizione e l'implementazione delle astrazioni. La seconda riguarda l'impiego di queste astrazioni per risolvere problemi reali.

\subsubsection{L'OS come gestore delle risorse}
La gestione delle risorse include il multiplexing (condivisione) delle risorse in due modalità diverse: nel tempo e nello spazio.Quando una risorsa è condivisa temporalmente programmi o utenti diversi fanno a turno ad usarla in un certo arco di tempo finito. L'altro tipo di multiplexing è nello spazio. Pressuponendo che vi sia abbastanza memoria per gestire parecchi programmi in memoria contemporaneamente, specialmente se necessita solo di una piccola frazione del totale. Tutto questo solleva però problemi di equità, protezione ecc\dots risolvibili dall'OS.
\pagebreak

\subsection{Analisi dell'hardware}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/1_3}}
        \caption{Alcuni dei componenti di un semplice personal computer}    
    \end{center}
    \end{figure}

    \subsubsection{System Bus}
    Inizialmente veniva usato un unico bus per gestire tutto il traffico. Combina le funzioni di: 
    \begin{itemize}
        \item Data bus effettivo di informazioni
        \item Address bus per determiare dove tali informazioni devono essere inviate
        \item Control bus per indicare quali operazioni dovrebbero essere eseguite
    \end{itemize}
    Sono stati aggiunti più bus dedicati per gestire il traffico da CPU a memoria e I/O, PCI, SATA, USB, ecc.
    
    \subsubsection{Dispostivi di I/O}
    Ogni dispositivo I/O è composto da due parti fondamentali il dispositivo fisico stesso e il controller del dispositivo (chip o set di chip che controllano una famiglia di dispositivi fisici).\\
    Il sistema operativo comunica con un controller del dispositivo utilizzando un driver di dispositivo specifico.\\
    La communicazione con i "Dispositivi di Controllo" ciacun controller del dispositivo ha un numero di registri dedicati per comunicare con esso:
    \begin{itemize}
        \item \textbf{Registri di stato}: forniscono alla CPU informazioni sullo stato del dispositivo I/O (ad esempio, inattivo, pronto per l'input, occupato, errore, transazione completata)
        \item \textbf{ Registri di configurazione/controllo}: utilizzati dalla CPU per configurare e controllare il dispositivo
        \item \textbf{Registri dei dati}: utilizzati per leggere o inviare dati al dispositivo I/O
    \end{itemize}
    La CPU riesce ad indirizzare mettendo l'address di un byte di memoria nel address bus, specificando il segnale \texttt{READ} sul controllo del bus. Alla fine, la RAM risponderà con il contenuto della memoria sul Data bus.
    
    La CPU può comunicare con un controller del dispositivo in due modi\footnote{\textbf{Port-mapped I/O}: utilizza uno spazio di indirizzamento separato (e istruzioni a livello macchina dedicate tipo IN/OUT) per la comunicazione tra CPU e i controller dei dispositivi di I/O (ossia, per riferirsi ai registri interni dei dispositivi di I/O). Si utilizzava prevalentemente nei vecchi sistemi per evitare di "sprecare" memoria principale, già di per sé limitata.\par
    \textbf{PMemory-mapped I/O}: dal punto di vista della CPU, comunicare con un registro interno di un dispositivo di I/O o con una locazione di memoria principale non c'è alcuna differenza! Una porzione della memoria principale sarà "riservata" per la comunicazione con i sottosistemi di I/O (sulla quale verranno mappati gli indirizzi dei registri dei dispositivi di I/O). Con questa soluzione, si spreca un po' di memoria ma si ha il vantaggio di non dover prevedere istruzioni dedicate per comunicare con i dispositivi di I/O; si possono usare le tradizionali LOAD/STORE che si utilizzano per leggere da/scrivere su locazioni di memoria. I sistemi moderni, tipicamente dotati di grandi quantitativi di memoria principale, prediligono questo secondo approccio perché lo spreco di memoria è trascurabile rispetto ai vantaggi che offre.}:
    \begin{itemize}
        \item \textbf{Port-mapped I/O}: I/O mappati alle porte che fanno riferimento ai registri del controller utilizzando un I/O separato
        spazio degli indirizzi. \\Il registro di ciascun controller del dispositivo I/O è mappato su una porta specifica (indirizzo).\\
        Richiede classi speciali di istruzioni CPU (ad es. IN/OUT). L'istruzione \texttt{IN} legge da un dispositivo I/O, mentre \texttt{OUT} scrive. Quando si usano le istruzioni IN o OUT, l' \texttt{M/\#IO} non viene asserito, quindi la memoria non risponde e il chip I/O sì.
        \item \textbf{Memory-mapped I/O}: I registri del controller del Memory-Mapped I/O utilizzano lo stesso spazio di indirizzamento utilizzato dalla memoria principale.\\Il Memory-mapped I/O "spreca" parte dello spazio degli indirizzi ma non necessita di istruzioni speciali. Per le porte del dispositivo I/O della CPU sono proprio come i normali indirizzi di memoria. La CPU utilizza istruzioni simili a \texttt{MOV} per accedere ai registri del dispositivo I/O. In questo modo viene asserito l' \texttt{M/\#IO} indicando l'indirizzo richiesto dalla CPU riferito alla memoria principale.
    \end{itemize}
    Per eseguire le attività di I/O, vengono utilizzate due modalità di gestione:
    \begin{itemize}
        \item Polling
        \item La CPU periodicamente verifica lo stato dei task delle attività di I/O
        \item Interrupt-driven dove la CPU riceve un segnale di interrupt dal controller una volta che la task I/O viene completata
        \item La CPU riceve un interrupt dal controller (device o DMA \footnote{Direct Memory Access}) una volta finito il task dell'I/O (con successo o in modo anomalo)
        \item La CPU svolge il lavoro effettivo di spostamento dei dati
        \item La CPU delega il lavoro a un controller DMA dedicato
    \end{itemize}

\subsubsection{Processori}
La CPU preleva le istruzioni della memoria (esegue il \textbf{fetch}) e le esegue. La CPU si occupa poi di decodificarla per determinare il tipo e gli operandi , eseguirla e poi prelevare, decodificare ed eseguire le successive. Poichè accedere alla memoria richiede molte risorse tutte le CPU contengono all'interno dei \textbf{registri}  per memorizzare variabili importanti o risultati temporanei. Oltre ai registri i computer contengono il \textbf{program counter (PC)}, contente l'indirizzo di memoria in sui si trova la successiva istruzione da seguire, dopodichè viene il PC viene aggiornato per posizionarsi sulla successiva. \\
Un altro registro è lo \textbf{stack pointer} che punta alla cima dello stack attuale. Un altro registro è il \textbf{program status word (PSW)} che contiene i bit di condizione, impostati da istruzioni di confronto, la prorità della CPU, la modalità (kernel o utente) e altri bit di controllo. 
Nelle architetture più moderne, vengono implementati più livelli di protezione, detti \textbf{protection rings}. Ogni ring aggiunge restrizioni sulle istruzioni eseguibili dalla CPU.
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/ring}}
        \caption{Protection Rings}    
    \end{center}
    \end{figure}
\pagebreak
Un modo semplice per avere una protezione alla memoria è quello di avere due registri dedicati:
\begin{itemize}
    \item \textbf{Base} $\rightarrow$ contenente il primo address di partenza 
    \item \textbf{Limite} $\rightarrow$ contenente l'ultimo address valido 
\end{itemize}
Il sistema operativo carica i rigistri di base e limite all'avvio del programma mentre la CPU controlla che ogni indirizzo di memoria a cui fa riferimento il programma utente rientri tra i valori base e limite.


Per ottenere servizi dall'OS un programma utente deve fare una \textbf{system call (chiamata di sistema)} che entra nel kernel e richiama l'OS. Quando il lavoro è stato completato il controllo è restituito al programma utente. Le chiamate di sistema devono essere implementate in spazio kernel.\\

L'istruzione \texttt{TRAP} cambia la modalità da utente a kernel e avvia l'OS. 
\begin{itemize}
    \item System call (software \texttt{TRAP}), ossia la richiesta di un servizio dell'OS, svolte in modo sincrono e innescate dai software
    \item Exception (fault), ossia la gestione di errori dovuti ad eventi inattesi, svolte in modo sincrono e innescate dai software
    \item Interrupt, ossia il completamento di una richiesta in attesa, svolte in modo asincrono e innescate dall'hardware
\end{itemize}

Esistono sei categorie importanti di System call:
\begin{itemize}
    \item \textbf{Gestione dei processi} include \texttt{end}, \texttt{abort}, \texttt{load}, \texttt{execute}, creazione e terminazione di processi, get/set attributi di processi, attesa, signal event, e allocazione di memoria libera. Quando un processo si interrompe, è necessario avviarne o riprenderne un altro. Quando i processi si arrestano in modo anomalo, potrebbe essere necessario fornire core dump e/o altri strumenti diagnostici o di ripristino
    \item \textbf{Gestione dei file} crea file, elimina file, apri, chiudi, leggi, scrivi, riposiziona, ottieni attributi di file e imposta attributi di file. Queste operazioni possono essere supportate anche per directory e file ordinari. L'effettiva struttura della directory può essere implementata utilizzando file ordinari sul file system o tramite altri mezzi (ne parleremo più avanti)
    \item \textbf{Gestione dei dispositivi} include dispositivi di richiesta, dispositivi di rilascio, lettura, scrittura, riposizionamento, recupero/impostazione degli attributi del dispositivo e collegamento o scollegamento logico dei dispositivi. I dispositivi possono essere fisici (ad es. unità disco) o virtuali/astratti (ad es. file, partizioni e dischi RAM). Alcuni sistemi rappresentano i dispositivi come file speciali nel file system, in modo che l'accesso al "file" richieda il driver di dispositivo del sistema operativo appropriato. E.g la directory \texttt{/dev} su qualsiasi sistema \texttt{UNIX}
    \item \textbf{Informazioni di sistema} include le chiamate per ottenere/impostare l'ora, la data, i dati di sistema e gli attributi di processo, file o dispositivo. I sistemi possono anche fornire la possibilità di eseguire il dump della memoria in qualsiasi momento. Programmi a passo singolo che interrompono l'esecuzione dopo ogni istruzione e tracciano
    il funzionamento dei programmi (debug). 
    \item \textbf{Communicazione} include creazione/eliminazione di connessioni di comunicazione, invio/ricezione di messaggi, trasferimento di informazioni sullo stato e collegamento/scollegamento di dispositivi remoti. Esisono due modelli di comunicazione: 
    \begin{itemize}
        \item \textbf{scambio di messaggi}\footnote{Nota: Più semplice e facile (in particolare per le comunicazioni tra computer) e generalmente appropriato per piccole quantità di dati\\}, il modello di trasmissione dei messaggi deve supportare le chiamate a:
        \begin{itemize}
            \item Identificare un processo remoto e/o un host con cui comunicare
            \item Stabilire una connessione tra i due processi
            \item Aprire e chiudere la connessione secondo necessità
            \item Trasmettere messaggi lungo la connessione
            \item Attendere i messaggi in arrivo, in stato di blocco o non blocco
            \item Eliminare la connessione quando non è più necessaria
        \end{itemize}
        \pagebreak
        \item \textbf{memoria condivisa}\footnote{Più veloce e generalmente l'approccio migliore in cui devono essere condivise grandi quantità di dati. Ideale quando la maggior parte dei processi deve leggere i dati anziché scriverli}, il modello di memoria condivisa deve supportare le chiamate a:
        \begin{itemize}
            \item  Creare e accedere alla memoria condivisa tra processi (e thread) 
            \item Fornire meccanismi di blocco che limitano l'accesso simultaneo
            \item Liberare memoria condivisa e/o allocarla dinamicamente secondo necessità
        \end{itemize}
        \end{itemize}
        \item \textbf{Protezione delle system call} Fornisce meccanismi per controllare quali utenti/processi hanno accesso a quali risorse di sistema. Le chiamate di sistema consentono di regolare i meccanismi di accesso secondo necessità. Agli utenti non privilegiati può essere concesso temporaneamente un accesso elevato autorizzazioni in circostanze specifiche. \par
        Quando un programma utente richiede l'esecuzione di una syscall tramite un'API fornita dal sistema operativo, tale richiesta viene prima convertita in linguaggio macchina, per poi venir gestita dal \textbf{System call Handler}, il quale si occuperà di salvare lo stato precedente dei registri, i quali verranno poi alterati durante l'esecuzione della syscall, per poi essere ripristinati. Inoltre il codice generico è indicizzato tramite la interrupt vector table (IVT). La \textbf{System-call table} contiene tante entry quante sono le chiamate di sistema supportate.
    \end{itemize}

\subsubsection{Passaggio di parametri}
Esistono tre metodi usati per passare parametri :
\begin{itemize}
    \item Salvare i parametri in registri
    \item Salvare parametri in \texttt{block} o \texttt{table} di un'area di memoria dedicata
    \item Parametri inseriti nello stack dal programma e estratti dallo stack dal sistema operativo (più complessi a causa dei diversi spazi degli indirizzi)
\end{itemize}
I metodi block e stack non limitano il numero o la lunghezza dei parametri passati
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.9\textwidth,keepaspectratio]{{im/ivt}}
        \caption{System call Handler}    
    \end{center}
    \end{figure}
\subsubsection{Timer, Istruzioni atomiche, Virtual Memory}
Il timer è una funzionalità hardware per abilitare la scheduling della CPU. Nei sistemi multi-tasking, permette alla CPU di non essere monopolizzata da processi "egoistici". Il timer genera un interrupt e ad ogni sua interruzione, lo scheduler della CPU prende il sopravvento e decide quale processo da eseguire successivamente.\\
Gli interrupt possono verificarsi in qualsiasi momento e interferire con i processi in esecuzione e l'OS deve essere in grado di sincronizzare le attività di cooperazione, simultanee processi, garantire che brevi sequenze di istruzioni (ad es. lettura-modifica-scrittura) vengano eseguite atomicamente da disabilitare gli interrupt prima della sequenza e riabilitarli successivamente. \\
La virtualizzazionde della memoria è un'astrazione (dell'effettiva memoria principale fisica) e conferisce ad ogni processo l'illusione che la memoria fisica sia solo contigua spazio degli indirizzi (spazio degli indirizzi virtuali). Permette di eseguire programmi senza che vengano caricati interamente nella memoria principale. Può essere implementata sia in HW (\textbf{MMU}) che in SW (OS). L'\textbf{allocazione contigua} è fondamentale per l'accesso sequenziale, evita la frammentazione e necessita il mantenimento dei blocchi liberi all'interno di una opportuna struttura dati.
\begin{itemize}
    \item \textbf{MMU}, associa gli indirizzi virtuali a quelli fisici tramite una tabella delle pagine gestita dal sistema operativo. Utilizza una cache denominata Translation Look-aside Buffer (TLB) con "mappature recenti" per ricerche più rapide. Il sistema operativo deve sapere quali pagine sono caricate nella memoria principale e quali su disco
    \item Il sistema operativo è responsabile della gestione degli spazi di indirizzi virtuali
\end{itemize}


\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.3\textwidth,keepaspectratio]{{im/virtualm}}
        \caption{Virtual vs. Physical Address Space}    
    \end{center}
    \end{figure}

Lo spazio degli indirizzi virtuali è tipicamente suddiviso in blocchi contigui della stessa dimensione (ad esempio, 4 KiB), chiamati pagine (\textbf{pages}). Ciascuna pagina che non sono caricate nella memoria principale vengono memorizzate su disco.

\pagebreak

\subsection{Progettazione e implementazione del sistema operativo}
La struttura interna dei diversi sistemi operativi può variare notevolmente e gli obiettivi del sistema è facilare l'usabilità rispetto al progettare/implementare. È fondamentale separare le politiche dai meccanismi della policy ovvero \textbf{cosa} sarà fatto e il meccanismo che indica \textbf{come} farlo.
Il disaccoppiamento della logica della politica dal meccanismo sottostante è un principio di progettazione generale nell'informatica, in quanto migliora il sistema:
\begin{itemize}
    \item flessibilità: l'aggiunta e la modifica delle politiche possono essere facilmente supportate
    \item riusabilità: i meccanismi esistenti possono essere riutilizzati per implementare nuove politiche 
    \item stabilità: l'aggiunta di una nuova politica non destabilizza necessariamente il sistema    
\end{itemize}
Le modifiche ai criteri possono essere facilmente regolate senza riscrivere il codice.

I primi sistemi operativi sviluppati in linguaggio assembly, e uno dei vantaggi era il controllo diretto sull'HW (alta efficienza) mentre uno svantaggio può essere il legame con uno specifico HW (bassa portabilità). Oggi abbiamo un misto di linguaggi e ai livelli più bassi troveremo assembly il corpo principale in C e i programmi di sistema in C, C++, linguaggi di scripting come PERL, Python, ecc.\\
Il sistema operativo dovrebbe essere suddiviso in sottosistemi separati, ciascuno con compiti, input/output e caratteristiche prestazionali accuratamente definiti. Esistono vari modi di strutturare un sistema operativo: 

\begin{itemize}
    \item \textbf{Struttura semplice}, dove non vi è alcun sottosistema e non vi è separazione tra kernel e user mode (esempio: il sistema MS-DOS). Semplice da implementare ma estremamente insicuro e poco rigido.
    \item  \textbf{Struttura a Kernel Monolitico}, dove l'intero sistema operativo opera in kernel mode e solo i software utente lavorano in user mode (esempio: il sistema UNIX). Semplice da implementare ed efficiente, ma ancora poco sicuro e rigido
    \item \textbf{Struttura a livelli} (come MULTICS) dove l'OS è suddiviso in N livelli ed ogni livello L usa funzionalità implementate dal livello $L_{1}$ ed espone nuove funzionalità al livello L + 1. Per via della struttura a livelli, il sistema è molto modulare, portabile e semplice da debuggare, rendendo tuttavia più complessa per la comunicazione tra di essi.
    \item \textbf{Struttura a Microkernel}, dove il kernel contiene solo le funzionalità di base, mentre tutte le altre funzionalità dell'OS e i programmi utente vengono eseguiti in user mode. Tale struttura porta ad una maggiore sicurezza, affidabilità ed estensibilità, ma anche ad un'efficienza ridotta.
    \item \textbf{Struttura a Moduli del Kernel caricabili (LKM)}, dove l'OS utilizza dei moduli tramite cui accedere alle funzionalità del kernel
    \item \textbf{Sistema a Kernel Ibrido}, dove viene utilizzato un approccio intermedio al kernel monolitico e al microkernel, ottenendo i vantaggi di entrambi gli approcci
\end{itemize}
\pagebreak
%-------------------------------------------------------%
\section{Gestione dei Processi e thread}

Un programma è un file eseguibile che risiede nella memoria persistente (ad es. disco) e contiene solo l'insieme di istruzioni necessarie per eseguire un lavoro specifico. Un processo è l'astrazione del sistema operativo di un programma in esecuzione (unità di esecuzione). Il processo è dinamico, mentre un programma è statico (solo codice e dati). Diversi processi possono eseguire lo stesso programma (ad esempio, multiple istanze di Google Chrome) ma ognuna ha il proprio stato. Un processo esegue un'istruzione alla volta, in sequenza. \\

Il sistema operativo fornisce la stessa quantità di spazio di indirizzi virtuali a ciascun processo. Lo spazio degli indirizzi virtuali è un'astrazione dello spazio degli indirizzi della memoria fisica. L'intervallo di indirizzi virtuali validi che un processo può generare dipende dalla macchina. Avremo perciò:
\begin{itemize}
    \item \textbf{Text} contenente le istruzioni dell'eseguibile
    \item \textbf{Data} Variabili globali e statiche inizializzate
    \item \textbf{BSS} variabile globale e statica (non inizializzata o inizializzata a 0)
    \item \textbf{Stack} Struttura LIFO utilizzata per memorizzare tutti i dati necessari a una chiamata di funzione (stack frame)
    \item \textbf{Heap} usata per l'allocazione dinamica
\end{itemize}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.3\textwidth,keepaspectratio]{{im/virtualaddress}}
        \caption{Virtual Address Space Layout}    
    \end{center}
    \end{figure}

Sullo stack sono definite due operazioni: \textbf{push} e \textbf{pop}. Il push viene usato per inserire degli elementi, mentre pop per rimuoverli. Usa un registro dedicato (e.g \texttt{esp}) il cui contenuto è l'indirizzo nella memoria principale della parte superiore dello stack. La memoria dello stack cresce convenzionalmente dall'alto verso il basso, cioè dagli indirizzi di memoria più alti a quelli più bassi. Ogni funzione utilizza una porzione dello stack, chiamata \textbf{stack frame} e in ogni momento possono esistere contemporaneamente più stack frame, a causa di diverse chiamate di funzioni nidificate, ma solo una è attiva.
Lo stack frame per ogni funzione è diviso in tre parti:
\begin{itemize}
    \item parametri della funzione + indirizzo di ritorno
    \item back-pointer allo stack frame precedente
    \item variabili locali
\end{itemize}
Il primo è impostato dal chiamante, il secondo e il terzo vengono impostati dal chiamato. Il puntatore \texttt{esp} viene sempre aggiornato man mano che lo stack cresce ed è difficile per il chiamato accedere ai parametri effettivi senza un riferimento fisso nello stack. Per risolvere questo problema invece di utilizzare un singolo puntatore in cima allo stack usa un puntatore aggiuntivo alla parte inferiore (base) dello stack (\texttt{\%ebp}) lascia che \texttt{esp} sia libero di cambiare tra diverse chiamate di funzione, mentre tiene \texttt{\%ebp} fissato all'interno di ogni stack frame. \\

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/ebp}}
        \caption{Function Parameters + Return}    
    \end{center}
    \end{figure}
\subsection{Stato di esecuzione del processo}
In ogni momento un processo può trovarsi in uno dei seguenti cinque stati:
\begin{itemize}
    \item \textbf{New}, il processo appena avviato
    \item \textbf{Ready}, il processo è pronto per essere eseguito ma attende di essere programmato sulla CPU
    \item \textbf{Running}, il processo sta effettivamente eseguendo istruzioni sulla CPU 
    \item \textbf{Waiting}, il processo è sospeso in attesa che una risorsa sia disponibile oppure fa richista di input da parte dell'utente
    evento da completare/verificare (ad es. input da tastiera, accesso al disco, timer, ecc.). Un processo è in esecuzione sulla CPU passa in stato waiting quando fa richiesta di input da parte dell'utente. 
    \item \textbf{Terminated}, il processo è terminato e il sistema operativo può distruggerlo
\end{itemize}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/diagramma}}
        \caption{Process Execution State Diagram}    
    \end{center}
    \end{figure}

Durante l'esecuzione, il processo passa da uno stato all'altro in base alle azioni del programma (ad esempio, chiamate di sistema) come azioni del sistema operativo (ad es. scheduling) o azioni esterne (ad es. interruzioni).\\ 

La maggior parte delle chiamate di sistema (ad esempio quelle di I/O) sono bloccanti. Il processo chiamante (spazio utente) non può fare nulla fino al ritorno della chiamata di sistema.\\
 Il sistema operativo (spazio del kernel) si occupa di:
\begin{itemize}
    \item impostare il processo corrente in uno stato di attesa (ovvero, in attesa del ritorno della chiamata di sistema)
    \item  pianifica un diverso processo pronto per evitare che la CPU sia inattiva
\end{itemize} 
Una volta che la chiamata di sistema ritorna, il processo precedentemente bloccato è pronto per essere schedulato nuovamente per l'esecuzione.\footnote{NOTA: l'intero sistema non è bloccato, solo il processo che ha richiesto la chiamata bloccata è!\\}\\
Lo stato del processo è costituito da: 
\begin{itemize}
    \item il codice del programma in esecuzione
    \item i dati statici del programma in esecuzione
    \item il program counter (PC) che indica la prossima istruzione da eseguire
    \item Registri CPU
    \item la catena di chiamate del programma (stack) insieme ai puntatori di frame e stack
    \item lo spazio per l'allocazione dinamica della memoria (heap) e al suo puntatore, le risorse in uso (ad es. file aperti)
    \item lo stato di esecuzione del processo (pronto, in esecuzione, ecc.)
\end{itemize}

\subsection{Process Control Block (PCB)}
Il \textbf{PCB} (o detto anche Tabella dei Processi), è la struttura dati principale utilizzata dal sistema operativo per tenere traccia di qualsiasi processo. Il PCB tiene traccia dello stato di esecuzione e della posizione di un processo. Il sistema operativo assegna un nuovo PCB alla creazione di un processo e lo inserisce in una coda di stato e viene deallocato non appena termina il processo associato. \\Associata a ogni classe di I/O c'è una posizione (solitamente in una collocazione fissa vicino alla base della memoria) chiamata \textbf{vettore di interrupt}\footnote{In informatica, un interrupt vector (vettore delle interruzioni) è un indirizzo di memoria del gestore di interrupt, oppure un indice ad un array, chiamato interrupt vector table, il quale può essere implementato tramite una dispatch table. La tabella degli interrupt vector contiene gli indirizzi di memoria dei gestori di interrupt. Quando si genera una interruzione, il processore salva il suo stato di esecuzione con il context switch, ed inizia l'esecuzione del gestore di interruzione all'interrupt vector (questo procedimento avviene quando l'interruzione ha carattere sincrono). Infatti vi è una wait instruction (istruzione d'attesa) che obbliga la CPU ad effettuare cicli vuoti fino a che non arriva il prossimo interrupt. Nel caso in cui si dovesse verificare una interruzione asincrona il programma provvede a lanciare un interrupt con la richiesta di I/O e nell'attesa del dato continua ad effettuare operazioni logico-aritmetiche. Quando arriverà nel momento in cui gli servirà il risultato si fermerà e inizierà ad aspettare. L'I/O asincrono serve a far sì che alcuni programmi possano anticipare la richiesta di un dato così nel caso in cui esso dovesse servire lo possono già utilizzare. Fonte: Wikipedia}. Contiene l'indirizzo della procedura di servizio dell'interrupt. Ad ogni interrupt il computer salta all'indirizzo specificato nel vettore dell'interrupt. 
Il PCB contiene:
\begin{itemize}
    \item Stato del processo pronto, in attesa, in esecuzione, ecc.
    \item Numero di processo (ovvero identificatore univoco)
    \item Program Counter (PC) + Stack Pointer (SP) + registri di uso generale 
    \item Informazioni sullo scheduling della CPU priorità e puntatori alle code di stato 
    \item Informazioni sulla gestione della memoria tabelle delle pagine
    \item Informazioni sull'account, ossia il  tempo utilizzato dalla CPU del kernel e dell'utente, stato I/O del proprietario 
    \item Elenco dei file aperti
\end{itemize}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.2\textwidth,keepaspectratio]{{im/PCB}}
        \caption{Process Execution State Diagram}    
    \end{center}
    \end{figure}
\pagebreak
\subsection{Creazione di processi}
I processi possono creare altri processi tramite specifiche chiamate di sistema. Il processo creatore è chiamato genitore del nuovo processo, chiamato figlio. Il genitore condivide risorse e privilegi con i suoi figli. Un genitore può aspettare che un figlio finisca o continuare in parallelo.\\
A ogni processo viene assegnato un identificatore intero (noto anche come identificatore di processo o PID) e per ogni processo viene memorizzato anche il PID genitore (PPID).\\
Sui tipici sistemi UNIX lo scheduler del processo è denominato \texttt{sched} e riceve il PID 0. La prima cosa che fa all'avvio del sistema è lanciare \texttt{init}, che dà a quel processo il PID 1. Successivamente \texttt{init} avvia tutti i daemons di sistema e i login degli utenti e diventa il genitore ultimo di tutti gli altri processi. I processi vengono creati attraverso la chiamata di sistema \texttt{fork()}.\\

Tutti i processi sono uguali. L'unico indizio di gerarchia dei processi è che quando viene creato un processo, al genitore viene dato uno speciale gettone o token (detto \textbf{handle}) che può controllare il figlio invalidando la gerarchia.\\
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/creazioneProcessi}}
        \caption{Process Creation: UNIX/Linux}    
    \end{center}
    \end{figure}
\pagebreak
\subsubsection{Risorse del padre vs figlio}
Abbiamo due possibilità per lo spazio degli indirizzi del figlio rispetto al genitore:
\begin{itemize}
    \item Il bambino può essere un duplicato esatto del genitore, condividendo lo stesso programma e gli stessi segmenti di dati in memoria
    \item Ciascuno avrà il proprio PCB, inclusi program counter, registri e PID 
    \item Questo è il comportamento della chiamata di sistema \texttt{fork()} in UNIX
    \item Il processo figlio può avere un nuovo programma caricato nel suo spazio degli indirizzi, con tutti i nuovi segmenti di codice e dati
    \item In Windows le chiamate di sistema saranno create con il comando \texttt{spawn()} 
    \item I sistemi UNIX implementano ciò come secondo passaggio, utilizzando la chiamata di sistema \texttt{exec}.
\end{itemize}
\subsubsection{Esecuzione del padre vs figlio}
Abbiamo due opzioni per il processo genitore dopo aver creato il figlio:
\begin{itemize} 
    \item Attendere che il processo figlio termini prima di procedere emettendo un \texttt{wait} chiamata di sistema, per un figlio specifico o per qualsiasi figlio
    \item Oppure eseguire contemporaneamente al figlio, continuando l'elaborazione senza essere bloccato (quando una shell UNIX esegue un processo come attività in background utilizzando "\&")
\end{itemize}

Consideriamo ora questo esempio:
\begin{lstlisting}
    #include <sys/types.h>
    #include <stdio.h>
    #include <unistd.h>
    int main(){
        pid_t pid;
        /* fork a child process */
        pid = fork();
        if (pid < 0) { 
            /* if the returned PID is < 0, an error occurred */
            fprintf(stderr, "Fork Failed");
            exit(-1); 
        }
        else if (pid == 0) {
        /* execute child process code */
            execlp("/bin/ls", "ls", NULL);
        }
        else {
        /* execute parent process code */
            ...
            /* wait for child to terminate */
            wait(NULL)
            printf("Child terminated");
            exit(0); 
    }
}
\end{lstlisting}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/p1}}
        \caption{Albero decisionale del primo esempio}    
    \end{center}
    \end{figure}
\pagebreak
Consideriamo ora quest'altro esempio:
\begin{lstlisting}
    int pid = fork();
    if(pid == 0) {      // A's child (B)
        pid = fork();
        
    if(pid == 0) {      // B's child (C)
        ...
        execlp(...);
    }
    else { // B 
        ...
    } 
}
else { // A 
    ...
}
\end{lstlisting}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.15\textwidth,keepaspectratio]{{im/p2}}
        \caption{Albero decisionale del secondo esempio}    
    \end{center}
\end{figure}

\pagebreak

Consideriamo ora questo esempio:
\begin{lstlisting}
    int pid = fork();
    if(pid == 0) {          // A's child (B)
        ...
        execlp(...);
    }
    else { // A
        pid = fork();
        if(pid == 0) {      // A's child (C)
            ...
            execlp(...);
        }
    }
\end{lstlisting}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.25\textwidth,keepaspectratio]{{im/p3}}
        \caption{Albero decisionale del secondo esempio}    
    \end{center}
\end{figure}

Consideriamo ora quest'altro esempio:
\begin{lstlisting}
    for(int i=0;i<n;i++) {
        if(fork() == 0) { // A0's child
            ...
            execlp(...);
        }
  }
  // back in the parent A0
  // wait for all children to terminate

  for(int i=0;i<n;i++) {
      wait(NULL);
  }
\end{lstlisting}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.2\textwidth,keepaspectratio]{{im/p4}}
        \caption{Albero decisionale del secondo esempio}    
    \end{center}
\end{figure}
Riepilogo delle chiamate di sistema viste:
\begin{itemize}
    \item \texttt{fork()} genera un nuovo processo figlio come copia esatta del genitore 
    \item \texttt{execlp} sostituisce il programma del processo corrente con il
    inserire il programma denominato
    \item \texttt{sleep} sospende l'esecuzione per un certo numero di secondi
    \item \texttt{wait}/\texttt{waitpid}  attende che uno o più processi specifici terminino l'esecuzione
\end{itemize}


\pagebreak

\subsection{Terminazione di processi}
I processi possono richiedere la propria terminazione effettuando la chiamata di sistema \texttt{exit}, tipicamente restituendo un \textit{int}. Questo \textit{int} viene passato al genitore se sta eseguendo un'attesa. Solitamente è 0 in caso di completamento con successo e un diverso da zero in caso di problemi.\\
I processi possono anche essere terminati dal sistema per una serie di motivi: 
\begin{itemize} 
    \item L'incapacità del sistema di fornire le risorse di sistema necessarie
    \item In risposta a un comando \texttt{kill} o ad un altro interrupt di processo non gestito
    \item Uscita normale (condizione volontaria)
    \item Uscita su errore (condizione volontaria)
    \item Terminato da un altro processo (condizione involontaria)
\end{itemize}
Un genitore può uccidere i suoi figli se il compito loro assegnato non è più necessario, il sistema però può consentire (o meno) al figlio di continuare senza un genitore. Sui sistemi UNIX, i processi orfani sono generalmente ereditati da \texttt{init}, che poi li uccide.\\

Quando un processo termina, tutte le sue risorse di sistema vengono liberate, i file aperti svuotati e chiusi, ecc. Lo stato di terminazione del processo dei tempi di esecuzione vengono restituiti al genitore se è in attesa che il figlio termini o \texttt{init} se il processo diventa orfano. \\

I processi sono detti \textbf{zombie} se stanno tentando di terminare ma non possono perché il loro genitore non li sta aspettando oppure ereditati da \texttt{init} come orfani e uccisi.

\subsection{Scheduling dei processi}

Gli obiettivi principali del sistema di schedulazione dei processi:
\begin{itemize} 
    \item mantenere la CPU sempre occupata
    \item fornire tempi di risposta "accettabili" per tutti i programmi, in particolare per quelli interattivi
\end{itemize}
Lo scheduler del processo deve soddisfare questi obiettivi implementando politiche adeguate per lo scambio dei processi dentro e fuori la CPU. Questi obiettivi però possono essere contrastanti, e ogni volta che il sistema operativo interviene per scambiare i processi, impiega tempo sulla CPU per farlo, che viene quindi "perso" dal fare qualsiasi lavoro produttivo utile.\\

Il sistema operativo mantiene i PCB di tutti i processi nelle code di stato. C'è una coda per ciascuno dei cinque stati in cui può trovarsi un processo e una per ogni dispositivo I/O (dove i processi attendono che un dispositivo diventi disponibile o consegni i dati). Quando il sistema operativo cambia lo stato di un processo, il PCB viene scollegato dalla coda corrente e spostato in quella nuova. Il sistema operativo può utilizzare criteri diversi per gestire ciascuna coda di stato. \\

La coda di esecuzione è vincolata dal numero di core disponibili sul sistema. Ogni volta, è possibile eseguire un solo processo su una CPU. Non esiste un limite teorico al numero di processi negli stati \texttt{new/ready/waiting/terminated}.\\

Esisono diversi scheduler (che in un sistema efficiente utilizza un mix tra questi):
\begin{itemize}
    \item Uno \textbf{scheduler a lungo termine} viene eseguito raramente ed è tipico di un sistema batch o di un sistema molto carico
    \item Uno \textbf{scheduler a breve termine} viene eseguito molto frequentemente (circa ogni 100 millisecondi) e deve sostituire molto rapidamente un processo dalla CPU e inserirne un altro
    \item Alcuni sistemi utilizzano anche uno \textbf{scheduler a medio termine}: quando i carichi del sistema diventano elevati, questo scheduler consente ai lavori più piccoli e più veloci di terminare rapidamente e liberare il sistema
\end{itemize}

\subsubsection{Il Context Switch}
Il \textbf{Context Switch} procedura utilizzata dalla CPU per sospendere il processo attualmente in esecuzione al fine di eseguirne uno pronto. È un'operazione molto costosa in quanto l'arresto del processo in corso implica il salvataggio di tutto il suo stato interno (PC, SP, altri registri, ecc.) al suo PCB e l'avvio di un processo pronto consiste nel caricare tutto il suo stato interno (PC, SP, altri registri, ecc.) dal suo PCB. \\
Il Context Switch si usa quando ci sono delle chiamate \texttt{TRAP}\footnote{richiesta di un servizio dell'OS, svolte in modo sincrono e innescate dai software\\} come system call, eccezioni, o interruzioni hardware. Ogni volta che arriva una \texttt{TRAP} la CPU ha il compito di creare uno stato di salvataggio dello stato corrente dei processi attivi. Passare alla modalità kernel per gestire l'interrupt ed eseguire un ripristino dello stato del processo interrotto. \\
Per evitare che i processi legati alla CPU monopolizzino la CPU, anche il Context Switch viene attivato tramite interrupt del timer hardware (\textbf{slice}).
Lo \textbf{slice} ha quantità massima di tempo tra due cambi di contesto per garantire che si verifichi almeno un cambio di contesto. Può accadere più frequentemente di così (ad esempio, a causa di richieste di I/O). Lo slice è facilmente implementabile in hardware tramite interrupt timer. E' un meccanismo utilizzato dai moderni sistemi operativi multitasking time-sharing per aumentare la reattività del sistema (pseudo-parallelismo). \footnote{Diverso invece quando si parla di \textbf{multiprocessore} che hanno due o più CPU che condividono la stessa memoria fisica, vero parallelismo hardware\\} \\
\subsubsection*{Il tempo nel Context Switch}
Il tempo impiegato per completare un cambio di contesto è solo tempo di CPU sprecato. Un intervallo di tempo inferiore comporta cambi di contesto più frequenti. Un intervallo di tempo maggiore comporta cambi di contesto meno frequenti. Ha anche il compito di minimizzare lo spreco di tempo della CPU, massimizzando quindi l'utilizzo e la reattività della CPU. 
\subsection{Communicazione dei processi}

I processi possono essere \textbf{indipendenti} o \textbf{cooperanti}:
\begin{itemize}
    \item Processi \textbf{indipendenti} operano contemporaneamente su un sistema e possono né influenzare né essere influenzati da altri processi
    \item I processi \textbf{cooperanti} possono influenzare o essere influenzati da altri processi al fine di raggiungere un compito comune
\end{itemize}

Possono esserci diversi processi che richiedono l'accesso allo stesso file (ad es pipeline). Abbiamo bisogno di questa cooperazione tra processi e ci sono diverse caratteristiche importanti. Abbiamo la \textbf{velocità} di calcolo è un problema che può essere risolta più velocemente se può essere suddiviso in sotto-attività da risolvere simultaneamente. La \textbf{modularità} definisce l'architettura più efficiente può essere quella di scomporre un sistema in moduli cooperanti. Infine abbiamo la \textbf{convenienza} infatti anche un singolo utente può essere multi-tasking, come modificare, compilare, stampare ed eseguire lo stesso codice in finestre diverse.
Ci sono due modi possibili per comunicare tra processi cooperanti:
\begin{itemize}
    \item \textbf{Memoria condivisa}\footnote{Per far sì che un processo possa richiedere un'area di memoria da condividere con altri processi c'è, ovviamente, necessità di effettuare una chiamata di sistema (a livello utente, non sarebbe possibile completare quest'operazione). Tuttavia, una volta "pagato questo prezzo iniziale", le interazioni successive tra processi che condividono un'area di memoria può avvenire senza ulteriori chiamate di sistema. \textbf{Fonte: il Prof}}
    \begin{itemize}
        \item Più complicato da configurare e non funziona altrettanto bene su più computer
        \item Più veloce una volta configurato, poiché non sono necessarie chiamate di sistema
        \item Preferibile quando (una grande quantità) di informazioni deve essere condivisa sullo stesso computer
    \end{itemize}
    \item \textbf{Scambio di messaggi} 
    \begin{itemize}
        \item Più semplice da configurare e funziona bene su più computer
        \item Più lento in quanto richiede chiamate di sistema (syscall) per ogni trasferimento di messaggi
        \item Preferibile quando la quantità e/o la frequenza dei trasferimenti di dati è piccola o quando sono multipli i computer sono coinvolti
    \end{itemize}
\end{itemize}

La memoria da condividere è inizialmente all'interno dello spazio degli indirizzi di un particolare processo e bisogna effettuare chiamate di sistema per rendere quella memoria pubblicamente disponibile ad altri processi.\\ Altri processi devono effettuare le proprie chiamate di sistema per collegare la memoria condivisa al proprio spazio degli indirizzi. \\
Il \textbf{Message Passing Systems} deve supportare almeno chiamate di sistema per l'invio e la ricezione di messaggi e stabilire un collegamento di comunicazione tra i cooperanti processi prima che i messaggi possano essere inviati.\\
\linebreak

Esistono però tre problemi da risolvere nello scambio di messaggi:\\

\begin{itemize}
    \item Comunicazione diretta o indiretta (es. naming) 
    \begin{itemize}
        \item Nella \textbf{Comunicazione diretta} il mittente deve conoscere il nome del destinatario a cui desidera inviare un messaggio. Deve conoscere anche il collegamento uno a uno tra ogni coppia mittente-destinatario per la comunicazione simmetrica e il destinatario deve conoscere anche il nome del mittente.
        \item La \textbf{Comunicazione indiretta} utilizza mailbox o porte condivise dove più processi possono condividere la stessa mailbox o porta. Solo un processo può leggere un determinato messaggio in una mailbox. Il sistema operativo deve fornire chiamate di sistema per creare ed eliminare mailbox e per inviare e ricevere messaggi da/per mailbox. 
    \end{itemize}
    \item Comunicazione sincrona o asincrona 
    \item Buffering automatico o esplicito
\end{itemize}
Scegliere se utilizzare o meno una queue per i messaggi:

\begin{itemize}
    \item \textbf{Zero capacity}, dove i messaggi non possono essere salvati in una queue, dunque il mittente rimane bloccato finché il destinatario non riceve il messaggio.
    \item \textbf{Bounded capacity}, dove vi è una queue di capienza predefinita, permettendo ai mittenti di non bloccarsi a meno che la queue non sia piena.
    \item \textbf{Unbounded capacity}, dove la queue ha teoricamente capienza infinita, implicando che i mittenti non debbano mai bloccarsi.
\end{itemize}


\subsection{Scheduling dei processi}
Definiamo come CPU burst lo stato in cui un processo è eseguito dalla CPU, mentre definiamo come I/O burst lo stato in cui un processo è in attesa che i dati vengano trasferiti dentro o fuori dal sistema.
In generale, ogni processo alterna costantemente tra CPU burst e I/O burst.\\
Lo scheduling dei processi è una \textbf{politica} che stabilisce quali processi devono essere eseguiti dalla CPU. Lo scheduler della CPU è un processo che, a seconda di una policy di scheduling stabilita, si occupa di selezionare un processo dalla ready queue da eseguire in ogni momento in cui la CPU è inattiva.
La struttura dati utilizzata per la ready queue e l'algoritmo usato per selezionare il processo successivo è basato su una queue FIFO (First In, First Out).

\begin{itemize}
    \item Concetti di base dello scheduling. Quasi tutti i programmi hanno alcuni cicli alternati di calcoli della CPU e attesa di I/O. Anche un semplice recupero dalla memoria principale richiede molto tempo rispetto alla velocità della CPU. (Consideriamo per ora la multiprogrammazione sui sistemi con un unico processore)
    \item Algoritmi di scheduling
    \item Concetti avanzati di scheduling.
\end{itemize} 

Ogni volta che la CPU diventa inattiva lo \textbf{scheduler} della CPU seleziona un altro processo dalla coda pronta per l'esecuzione successiva. La struttura dei dati per la coda pronta e l'algoritmo utilizzato per selezionare il processo successivo non sono però necessariamente basati su una coda FIFO. Le \textbf{decisioni di scheduling} della CPU avvengono in una delle quattro condizioni:
\begin{enumerate}
    \item Quando un processo passa dallo stato in \textbf{esecuzione} allo stato di \textbf{attesa}
    \item Quando un processo passa dallo stato in \textbf{esecuzione} allo stato \textbf{pronto}
    \item Quando un processo passa dallo stato di \textbf{attesa} allo stato \textbf{pronto}
    \item Quando un processo viene \textbf{creato} o \textbf{terminato}
\end{enumerate}

Nel 1° e il 4° caso è necessario selezionare un nuovo processo, mentre nel 2° e 3° caso si continua con il processo corrente o bisogna selezionarne uno nuovo. 
\subsection{Scheduling Preemptive e Non-Preemptive}
Un algoritmo di scheduling \textbf{non-preemptive} assegna la CPU a un processo e non lo interrompe finché non termina o finché non rilascia volontariamente la CPU. Ciò significa che un processo che richiede una quantità considerevole di tempo della CPU può bloccare altri processi che sono in attesa di esecuzione.\par
Un algoritmo di scheduling \textbf{preemptive}\footnote{Nota: uno scheduler preemptive viene comunque attivato dalle condizioni 1 e 4, poiché in tali casi deve comunque essere selezionato un processo.}, al contrario, consente al sistema operativo di interrompere l'esecuzione di un processo in qualsiasi momento per consentire l'esecuzione di un altro processo in coda. Questo significa che i processi con un tempo di esecuzione più breve possono essere eseguiti più rapidamente, anche se ci sono altri processi in attesa di esecuzione. In caso di un scheduling preemptive lo scheduler passa dai stati sia di running che di waiting per passare agli stati di ready o waiting stesso.  
\subsubsection{Problemi}
Preemption potrebbe causare problemi se si verifica mentre il kernel è impegnato nell'implementazione di una system call (ad esempio, l'aggiornamento dei dati critici del kernel strutture) oppure due processi condividono dati, uno può essere interrotto durante l'aggiornamento di strutture dati condivise. Ci sono però possibili \textbf{soluzioni}, si può fare in modo che il processo attenda finché la chiamata di sistema non è stata completata o bloccata prima di concedere la preemption (problematico per i sistemi in tempo reale, poiché la risposta in tempo reale non può più essere garantita) oppure disabilitare gli interrupt prima di entrare nella sezione del codice critico e riabilitarli subito dopo (dovrebbe essere fatto solo in rare situazioni e solo su parti di codice molto brevi che finiranno rapidamente). \\
\subsection{Il Dispatcher}
Il dispatcher è il modulo che dà il controllo della CPU al processo selezionato dallo scheduler. Esso legge le richieste di lavoro in arrivo e dopo aver esaminato la richiesta, sceglie un \textbf{thread worker inattivo} (cioè bloccato) e gli fa gestier la richiesta scrivendo eventualmente un puntatore al messaggio all'interno di una parola seciale associata a ogni thread. Il dispatcher risveglia così il worker dormiente facendo passare dallo stato di "\textit{bloccato}" a quello di "\textit{ready}".\\
Ha il compito di:
\begin{itemize}
    \item Cambio di contesto
    \item Passaggio alla modalità utente
    \item Saltare alla posizione corretta nel programma appena caricato
    \item Il dispatcher viene eseguito su ogni cambio di contesto quindi il tempo che esso consuma (latenza di invio) deve essere il più breve possibile
\end{itemize}

\subsection{Definizioni utili}

\begin{itemize} 
    \item Orario di arrivo: ora in cui il processo arriva nella coda pronta
    \item Tempo di completamento (Arrival Time): momento in cui il processo completa la sua esecuzione
    \item Burst Time: tempo richiesto da un processo per l'esecuzione della CPU
    \item TurnaroundTime: differenza di tempo tra il completamento e l'orario di arrivo 
    \item Waiting Time: differenza di tempo tra il tempo di turnaround e il burst time
\end{itemize}
\pagebreak

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/formule}}
        \caption{}    
    \end{center}
    \end{figure}
\subsection{Criteri/metriche di scheduling}
Esistono diversi criteri da considerare quando si tenta di selezionare l'algoritmo di scheduling "migliore", e deve prendere in considerazione:
\begin{itemize}
    \item Utilizzo della CPU, ossia la percentuale di tempo in cui la CPU è occupata
    Idealmente la CPU sarebbe occupata il 100\% del tempo, in modo da sprecare 0 cicli CPU
    Su un sistema reale, l'utilizzo della CPU dovrebbe variare dal 40\% (con carico leggero) al 90\% (con carico pesante)
    \item Throughput, ossia il numero di processi completati in un'unità di tempo. 
    \item Tournaround time, ossia il tempo necessario per il completamento di un particolare processo, dall'invio al completamento. Include tutto il tempo di attesa
    \item Waiting time, ossia il tempo necessario ai processi trascorrono nella coda pronta aspettando il proprio turno per salire sulla CPU. I processi nello stato di attesa non sono sotto il controllo dello scheduler della CPU (semplicemente non sono pronti per essere eseguiti).\footnote{Da non confondere con lo waiting state}
    \item Response time, ossia il tempo impiegato dall'emissione di un comando all'inizio di una risposta a tale comando. Si usa principalmente, per processi interattivi
\end{itemize}

Idealmente, si sceglie uno scheduler della CPU che ottimizzi tutte le metriche contemporaneamente infatti quanto è stado detto sopra è impossibile ed è necessario un compromesso. L'obiettivo era di scegliere un algoritmo di pianificazione in base alla sua capacità di soddisfare una data politica.\\
L'obiettivo dello scheduling è di ridurre al minimo il tempo medio di risposta\footnote{Tipico dei sistemi interattivi}:
\begin{itemize} 
    \item Minimizzare il \textbf{tempo medio} di risposta
        \item Fornire l'output all'utente il più rapidamente possibile
    \item Ridurre al minimo il tempo di risposta massimo
        \item Limite nel caso peggiore
    \item Ridurre al \textbf{minimo} la varianza del tempo di risposta
        \item Gli utenti accettano di più un sistema coerente e prevedibile piuttosto che uno incoerente
\end{itemize}

\pagebreak

Massimizzare il throughput significa\footnote{Tipico dei sistemi batch. I \textbf{sistemi batch} (o "batch processing systems" in inglese) sono un tipo di sistema informatico che esegue una serie di lavori o processi in una sequenza predefinita, senza interazione diretta con l'utente o l'operatore del sistema.
In un sistema batch, i lavori vengono raccolti in un'area di elaborazione chiamata "coda di lavoro" o "batch queue". Il sistema elabora poi i lavori uno per uno, seguendo un ordine predefinito, e può eseguirli in background, senza richiedere l'input dell'utente durante l'esecuzione. Fonte: Wikipedia}:
\begin{itemize}
    \item Riduzione al minimo dell'overhead (cambio di contesto del sistema operativo)
    \item Utilizzo efficiente delle risorse di sistema (CPU e dispositivi I/O)
    \item Ridurre al minimo i tempi di attesa
    \item Dare a ogni processo la stessa quantità di tempo sulla CPU 
    \item Potrebbe aumentare il tempo medio di risposta
\end{itemize}


Per le politiche di scheduling assumiamo che:
\begin{itemize}
    \item Vi è un singolo processo per utente
    \item I processi sono indipendenti tra di loro, dunque non vi è comunicazione 
    \item Ogni processo è costituito da un singolo thread
\end{itemize}

\subsection{Algoritmi di Scheduling}
\subsubsection{First-Come-First-Serve (FCFS)}
Il FCFS ha solo una coda FIFO, lo scheduler esegue i lavori fino al completamento nell'ordine di arrivo ed entra in azione solo quando il lavoro attualmente in esecuzione lo richiede un'operazione di I/O (o termina la sua esecuzione). Un lavoro può continuare a utilizzare la CPU a tempo indeterminato (ovvero fino a quando non si blocca) per questo è di tipo Non-preemptive. 
\pagebreak
\subsubsection{Esempi}
\begin{enumerate}
    \item Consideriamo la seguente coda di processi: 
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs1}}
            \caption{}    
        \end{center}
    \end{figure}
    \begin{itemize}
        \item I processi vengono creati allo stesso istante, dunque l'arrival time per tutti i processi è l'istante 0. Successivamente, tutti e tre i processi vengono messi nella ready queue
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs12}}
                \caption{}    
            \end{center}
        \end{figure}
        \item Di seguito, lo scheduling FCFS seleziona il primo processo della ready queue, ossia il processo A, spostandolo in stato di ready e completando la sua esecuzione
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs13}}
                \caption{}    
            \end{center}
        \end{figure}
        \pagebreak
        \item Una volta terminato il processo A, lo scheduler ripeterà le stesse operazioni fino a che tutti i processi non saranno terminati
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs14}}
                \caption{}    
            \end{center}
        \end{figure}
        \item Una volta terminati tutti i processi, calcoliamo il waiting time medio:
        \begin{itemize}
            \item Per il processo A si ha: \\
            $T_{waiting} = T_{turnaround} - T_{burst} = T_{completion} - T_{arrival} - T_{burst} = 5 - 0 - 5 = 0$
            \item Per il processo B si ha:\\
            $T_{waiting} = T_{turnaround} - T_{burst} = T_{completion} - T_{arrival} - T_{burst} = 7 - 0 - 2 = 5$
            \item Per il processo C si ha:\\
            $T_{waiting} = T_{turnaround} - T_{burst} = T_{completion} - T_{arrival} - T_{burst} = 10 - 0 - 3 = 7$
            \item Il waiting time medio, quindi, sarà:
            \begin{center}
                $\bar{T}_{waiting} = \frac{1}{n}\sum\limits_{i=0}^{n} T_{i}^{waiting}$ = $\frac{0 + 5 + 7}{3} = \frac{12}{3} = 4$
            \end{center}
        \end{itemize}
    \end{itemize}
    \item Nel seguente scenario, utilizzando sempre uno scheduling FCFS, si ha:
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs21}}
            \caption{}    
        \end{center}
    \end{figure}
    \begin{itemize}
        \item Il waiting time medio, quindi, sarà:
        \begin{center}
            $\bar{T}_{waiting} = \frac{1}{n}\sum\limits_{i=0}^{n} T_{i}^{waiting}$ = $\frac{0 + 5 + 2}{3} = \frac{7}{3} \simeq 2.3$
        \end{center}
    \end{itemize}
\pagebreak
    \item Consideriamo il seguente scenario:
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs31}}
            \caption{}    
        \end{center}
    \end{figure}
    
    \begin{itemize}
        \item Supponiamo che dopo 2 unità temporali il processo A esegua una richiesta I/O, entrando quindi in stato di waiting, la quale verrà completata dopo un istante.
        Attenzione: poiché il tempo in attesa I/O non viene calcolato nel waititng time, sarà necessario sottrarre un istante dal waiting time del processo A.
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs32}}
                \caption{}    
            \end{center}
        \end{figure}
        \item Una volta entrato in stato di waiting, lo scheduler selezionerà il processo seguente nella coda, ossia il processo B, portandolo a termine
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs33}}
                \caption{}    
            \end{center}
        \end{figure}
        \pagebreak
        \item Una volta terminata l'esecuzione del processo B, il processo A verrà selezionato per riprendere l'esecuzione.
        Attenzione: viene selezionato il processo A e non il processo C poiché l'algo- ritmo FCFS utilizza una queue basata sull'arrival time dei processi
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs34}}
                \caption{}    
            \end{center}
        \end{figure}
        \item Infine, il waiting time medio sarà:
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/fcfs35}}
                \caption{}    
            \end{center}
        \end{figure}
        \\
        $\bar{T}_{waiting} = \frac{1}{n}\sum\limits_{i=0}^{n} T_{i}^{waiting}$ = $\frac{1 + 2 + 7}{3} = \frac{10}{3} \simeq 3.3$
    \end{itemize}
\end{enumerate}
\subsubsection*{Pro e Contro}
Uno dei \textbf{vantaggi} è la semplicità di implementazione.\\
Tra gli \textbf{svantaggi}:
\begin{itemize} 
    \item Il tempo di attesa (medio) è molto variabile in quanto i lavori di breve durata della CPU possono rimanere indietro quelli molto lunghi.
    \item effetto convoglio: scarsa sovrapposizione tra CPU e I/O poiché i lavori legati alla CPU costringeranno i lavori legati all'I/O ad attendere.
\end{itemize}
\pagebreak
\subsubsection{Round - Robin (RR)}

Il Round Robin è un algoritmo di scheduling \textbf{preemptive} dove viene selezionato il primo processo presente in ready queue dove i burst della CPU sono assegnati con limiti chiamati \textbf{time quantum} o \textbf{(time slice)}. 

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/timeslice}}
        \caption{}    
    \end{center}
\end{figure} 

Quando un lavoro viene assegnato alla CPU, viene impostato un timer per un determinato valore, se il lavoro termina prima della scadenza del quanto temporale, viene sostituito da CPU proprio come il normale algoritmo FCFS. Se il timer si spegne per primo, il lavoro viene scambiato fuori dalla CPU e spostato nel back-end della coda dei pronti.\\
Viene utilizzato in molti sistemi di time-sharing in combinazione con gli interrupt del timer. \\
La queue pronta viene mantenuta come queue circolare. Quando tutti i lavori hanno avuto un turno, lo scheduler ne assegna un altro al primo lavoro girare e così via. L'algoritmo RR viene considerato equo in quanto condivide la CPU equamente tra tutti i lavori. Il tempo medio di attesa può essere più lungo rispetto ad altri algoritmi di scheduling.\\
Il limite superiore per lo start time di un processo in una coda di n processi utilizzando uno scheduler RR corrisponde a $sup\{T_{i}^{start}\}$ = $\delta$ $\cdot$ (i - 1), dove i $\in$ $[1,n]$ e dove $\delta$ è il time slice.

\subsubsection{Esempi}
\begin{itemize}
    \item Consideriamo la seguente coda dei processi utilizzando un algoritmo RR con un time slice di 2 e un context switch trascurabile:
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/rr1}}
            \caption{}    
        \end{center}
    \end{figure}
    \item Il primo processo a prendere il controllo della CPU è il processo A, venendo bloccato ed aggiunto alla ready queue dopo 2 unità temporali per via del time slice
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/rr12}}
            \caption{}    
        \end{center}
    \end{figure}
    \pagebreak
    \item Successivamente, il processo B prende controllo della CPU, venendo anch'esso bloc- cato dopo 2 unità temporali. In questo caso, tuttavia, il processo B non verrà aggiunto alla fine della ready queue poiché la sua esecuzione è terminata
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/rr13}}
            \caption{}    
        \end{center}
    \end{figure}
    \item Di seguito, il processo C prenderà il controllo, venendo bloccato dopo 2 tempi
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/rr14}}
            \caption{}    
        \end{center}
    \end{figure}
    \item Infine, vengono eseguiti i processi A e C finché essi non verranno completati
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/rr15}}
            \caption{}    
        \end{center}
    \end{figure} 
    \item Il waiting time medio, quindi, sarà:\par
    \begin{equation}
        \bar{T}_{waiting} = \frac{1}{n}\sum\limits_{i=0}^{n} T_{i}^{waiting} = \frac{5 + 2 + 6}{3} = \frac{13}{3} \simeq 4.3
    \end{equation}
\end{itemize}
\pagebreak
\subsubsection*{FCFS vs RR}
Confrontando il tournaround time e il waiting time medio tra uno scheduler FCFS e uno scheduler RR, il primo sembra essere a primo occhio più performante del secondo. \par
Tuttavia, considerando la varianza tra i waiting time di ogni processo, notiamo come il RR risulta essere più equo, fornendo ad ogni processo la stessa quantità di tempo di utilizzo della CPU.
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/fcfsrr}}
        \caption{}    
    \end{center}
\end{figure} 

\pagebreak

\subsubsection{Shortest-Job-First (SJF)}
Il Shortest-Job-First è u altro algoritmo di scheduling di tipo \textbf{non preemptive} che parte dal presupposto che i tempi di esecuzione siano conosciuti in anticipo. Lo scheduler preleva per primo il lavoro più breve (da qui il nome \textbf{shortest job first}). \par
Una versione \textbf{preemptive} dell'algoritmo shortest job first è l'algoritmo shortest remaining time first dove ogni volta che un nuovo job arriva nella ready queue e il suo CPU burst stimato è minore di quello rimanente del job attualmente in esecuzione, tale job prende controllo della CPU.\par
Lo SJF presenta dei vantaggi infatti risulta ottimale quando l'obiettivo è minimizzare il tempo medio di attesa. Funziona sia con schedulatori preventivi che non preventivi.\par
Tra gli svantaggi possiamo affermare che risulta quasi impossibile prevedere la quantità di tempo CPU di un lavoro. Anche i lavori legati alla CPU a esecuzione prolungata possono morire di \textit{starve} (come hanno fatto implicitamente quelli legati all'I/O priorità maggiore rispetto a loro).\par
Per stimare la lunghezza del prossimo CPU burst, viene utilizzato l'exponential smoothing:
\begin{itemize}
    \item Sia $x_{t}$ la lunghezza effettiva del t-esimo CPU burst
    \item Sia $S_{t+1}$ la lunghezza stimata del $(t + 1)$-esimo CPU burst 
    \item Sia $\alpha$ $\in$ $R$ dove 0 $\geq \alpha \leq$ 1
    \item Si ha che:
    \begin{center}
        $s_{1}$ = $x_{0}$\\
        $s_{t+1} = \alpha x_{t} +(1 - \alpha) s_{t}$
    \end{center}
\end{itemize}

\subsubsection{Esempi}
\begin{enumerate}
    \item Consideriamo la seguente coda di processi gestita da uno scheduler \textbf{SJF non preemptive}. In tal caso, si ha che:\\
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/sjf1}}
                \caption{}    
            \end{center}
        \end{figure} \\
        \begin{center}
            $\bar{T}_{ave.waiting}$ = $\frac{3 + 16 + 9 + 0}{4}$ = $\frac{28}{4} = 7$
        \end{center}
    \pagebreak
    \item Consideriamo la seguente coda di processi gestita da uno scheduler \textbf{SJF preemptive}, ossia uno scheduler \textbf{SRTF}.
    \begin{itemize}
        \item \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/sjf2}}
                \caption{}    1
            \end{center}
        \end{figure} 
        \item Il primo job ad essere eseguito è il processo A, poiché il primo ad essere creato ed inserito nella ready queue.
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/sjf21}}
                \caption{}    
            \end{center}
        \end{figure} 
        \item Una volta che il processo B viene creato, il suo CPU burst stimato, ossia 4, viene comparato con il CPU burst rimanente del processo A, ossia 7. Poiché 4 $<$ 7, allora il processo B prende controllo della CPU.
        Analogamente, nell'istante in cui il processo C viene creato, il suo CPU burst stimato, ossia 9, viene comparato con quello rimanente del processo A, ossia 6, e quello del processo B, ossia 3. Siccome 3 $<$ 7 $<$ 9, allora il processo B mantiene il controllo della CPU.
        Lo stesso ragionamento viene effettuato anche dopo la creazione del processo D, dove il processo B mantiene il controllo \hbox{(2 $<$ 7 $<$ 5 $<$ 9)}.\\ Di conseguenza, il processo B verrà eseguito fino al suo completamento.
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/sjf22}}
                \caption{}    
            \end{center}
        \end{figure} 
        \pagebreak
        \item Una volta completato il processo B, verrà eseguito il processo avente il CPU burst stimato minore:
        \begin{itemize}
            \item  Il processo A ha un CPU burst rimanente pari a 7 
            \item Il processo C ha un CPU burst rimanente pari a 9 
            \item Il processo D ha un CPU burst rimanente pari a 5
            Di conseguenza, verrà eseguito il processo D fino al suo completamento
        \end{itemize}
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/sjf23}}
                \caption{}    
            \end{center}
        \end{figure} 
        \item Analogamente al processo D, verranno eseguiti il processo A e il processo C, ognuno di essi fino al loro completamento (poiché, nel frattempo, nessun altro processo viene inserito nella ready queue, dunque non viene mai attivato l'algoritmo di scheduling).
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/sjf24}}
                \caption{}    
            \end{center}
        \end{figure}
        \begin{center}
            
        $\bar{T}_{ave.waiting}$ = $\frac{(17 - 0 - 8)+(5 - 1 - 4)+(26 - 2 - 9)+(10 - 3 - 5)}{4}$ = $6.5$
    \end{center}
    \end{itemize}
\end{enumerate}

\subsubsection*{Confronto tra FCFS, RR e SJF}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/confronto3}}
        \caption{}    
    \end{center}
\end{figure}
\pagebreak

\subsubsection{Priority Scheduling}
Nel \textbf{Priority Scheduling} a ciascun processo è assegnata una priorità e il processo eseguibile con la priorità più alta è quello cui è consentita l'esecuzione. Per impedire che i processi ad alta priorità siano eseguiti \textbf{indefinitamente}, lo scheduler può abbassare la priorità del processo attualmente in esecuzione a ogni scatto del clock (cioè a ogni interrupt). Se questa azione fa sì che la sua \textbf{priorità} vada al di sotto di quella del proceso successivo, avviene uno scambio di processo. In alternativa a ciascun processo può essere assegnato uno slice massimo in cui può essere eseguito.\par 
Le priorità possono essere assegnate in due modi:
\begin{itemize}
    \item Internamente, ossia assegnate dall'OS in base a determinati criteri (ad esempio il CPU burst medio, il rateo di attività tra CPU e I/O, risorse utilizzate, \dots)
    \item Esternamente, ossia assegnate dall'utente in base all'importanza del job o di altri criteri.
\end{itemize}\par

Un semplice algoritmo che offre un buon servizio ai processi I/O bound è quello di impostare la priorità a 1/$f$, dove $f$ è la frazione dell'ultimo quanto usao dal processo. Un processo che ha usato uno solo dei suoi 50 ms di slice avrà una prioria 50, mentre un processo che sia stato eseguito per 25 ms prima di bloccarsi avrà priorità 2; un processo che abbia usato tutto lo slice avrà priorità 1.

Il priority scheduling può essere sia \textbf{non-preemptive} sia \textbf{preemptive}.

\subsubsection*{Problemi}
Il Priority Scheduling può soffrire di \textit{starvation}, dove un job di bassa priorità rimane in attesa per un tempo indefinito, poiché altri job hanno sempre una priorità maggiore. Tali job potrebbero essere eseguiti eventualmente quando il carico del sistema è minore o dopo che il sistema stesso vada in crash, venga spento o venga riavviato. 
Come contromisura alla \textit{starvation} viene utilizzato l'\textit{aging}, ossia l'incremento della priorità di un job in base al suo tempo in attesa, finché essi non verranno eventualmente schedulati.
\pagebreak
\subsubsection{Multilevel Queue (MLQ e MLFQ)}
L'algoritmo \textbf{Multilevel Queue (MLQ)} è un algoritmo di scheduling basato sull'utilizzo di queue multiple separate tra loro, ognuna per ogni categoria di job, dove ogni queue utilizza l'algoritmo di scheduling più appropiato per una determinata categoria di job. Una volta inserito all'interno di una queue, nessun job può essere spostato in un'altra queue.\par
Le due opzioni più comuni per l'implentazione di tale algoritmo sono:
\begin{itemize}
    \item \textbf{String priority}, dove nessun job all'interno di una queue di priorità più bassa viene eseguito finché esiste almeno un job delle queue di priorità più alta
    \item \textbf{Round robin}, dove ogni queue utilizza un proprio time slice, il quale aumenta esponenzialmente al diminuire della priorità della coda
\end{itemize}
L'algoritmo \textbf{Multilevel Feedback Queue} (MLFQ) segue la stessa idea dell'algoritmo MLQ, con l'aggiunta della possibilità per ogni job di essere spostato da una queue all'altra.\par
Lo spostamento di un job può rivelarsi necessario quando:
\begin{itemize}
    \item Un job passa dall'utilizzare molto la CPU all'utilizzare molto l'I/O e viceversa
    \item Un job è in stato di starving, venendo spostato per breve tempo in una queue a priorità più alta
\end{itemize}
Per gestire gli spostamenti, l'algoritmo utilizza le seguenti regole:
\begin{itemize}
    \item Inizialmente ad ogni job viene assegnata la priorità più alta
    \item Se il time slice di un job scade, allora quest'ultimo viene spostato nella queue con un livello di priorità inferiore rispetto a quella precedente
    \item Se il time slice di un job non scade, allora quest'ultimo viene spostato nella queue con un livello di priorità superiore rispetto a quella precedente
    \item La priorità di processi strettamente legati alla CPU viene diminuita rapidamente
    \item La priorità di processi strettamente legati all'I/O rimarrà alta
\end{itemize}

\subsubsection{Esempio di suddivisione tramite MLQ e MLFQ:}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/mlq}}
        \caption{}    
    \end{center}
\end{figure}
\pagebreak
\subsubsection{Esempio}
\begin{enumerate}
    \item  Consideriamo la seguente coda di processi gestita da un algoritmo MLFQ con 3 queue e strict priority.
    \begin{itemize}
        \item Supponiamo:
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/mlq1}}
                \caption{}    
            \end{center}
        \end{figure}
        \item Per indicare l'avanzamento di ogni processo all'avanzare del tempo, utilizziamo la notazione:
        \begin{center}
            $JOB_{total\_elapsed\_time}^{execution\_time}$
        \end{center}
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.55\textwidth,keepaspectratio]{{im/mlq2}}
                \caption{}    
            \end{center}
        \end{figure}
        \item Siccome il time slice per tutti e tre i processi è scaduto, ognuno di essi viene spostato nella queue a priorità inferiore
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.55\textwidth,keepaspectratio]{{im/mlq3}}
                \caption{}    
            \end{center}
        \end{figure}
        \pagebreak
        \item Analogamente, a prima, anche in questo caso il time slice per tutti e tre scade, dunque vengono spostati alla queue inferiore
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/mlq4}}
                \caption{}    
            \end{center}
        \end{figure}
        \item Una volta raggiunta la queue a priorità più bassa, il time slice di tutti e tre i processi scadrà sempre fino al loro completamento
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/mlq5}}
                \caption{}    
            \end{center}
        \end{figure}
    \end{itemize}
\end{enumerate}

\subsubsection{Lottery scheduling}
L'algoritmo \textbf{lottery scheduling} è un algoritmo di scheduling basato sulla casualità:
\begin{enumerate}
    \item Ad ogni job vengono assegnati un determinato numero di biglietti. I job attivi da poco riceverrano più biglietti, mentre quelli attivi da molto ne riceveranno di meno. Per evitare la starvation, ad ogni job viene assegnato almeno un biglietto
    \item Ogni volta che un time slice scade viene determinato un biglietto vincitore. Il processo che detiene tale biglietto prenderà il controllo della CPU.
    \item Ai processi più importanti vengono assegnati biglietti extra
    \item Successivamente, il procedimento viene ripetuto
\end{enumerate}
Per via della legge dei grandi numeri, all'aumentare del numero di estrazioni effettuate il tempo di utilizzo della CPU di ogni processo tenderà a raggiungere la media.


\pagebreak
\subsection{Thread e Multi-threading}
Un \textbf{thread} è un'unità base dell'utilizzo della CPU e consiste in un contatore di programma, uno stack, e un insieme di registri oltre che un proptio ID. Ogni processo definisce le risorse "globali" (ossia lo spazio d'indirizzamento, le istruzioni da eseguire, i dati, le risorse, \dots) mentre ogni suo thread definisce un singolo stream di esecuzione all'interno del processo stesso.
Poiché lo spazio d'indirizzamento del processo è condiviso tra tutti i suoi thread, nessuna syscall è richiesta per far cooperare i thread tra di loro, risultando in una comunicazione più semplice rispetto allo scambio di messaggi e alla memoria condivisa.

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.55\textwidth,keepaspectratio]{{im/thr}}
        \caption{}    
    \end{center}
\end{figure}
Possiamo notare infatti che:
\begin{itemize}
    \item Ciascun thread possiede un insieme di registrasti indipendenti e un proprio stato
    \item Tutti i thread di processi condividono lo stesso codice e le risorse globali
    \item Da quando tutti i thread sono attivi nello stesso spazio di indirizzamento la communicazione tra loro risulta più semplice della communicazione tra i processi
\end{itemize}

I thread sono molto utili nella programmazione moderna ogni volta che un processo ha più attività per eseguire indipendentemente dagli altri. Ciò è particolarmente vero quando una delle attività può bloccare, ed è desiderato consentire alle altre attività di procedere senza bloccare (E.g: elaboratore di testi). Un thread può verificare l'ortografia e la grammatica mentre un altro thread gestisce l'input dell'utente (tasti) e un terzo fa backup periodici del file che viene modificato. \par
Teoricamente, ogni attività secondaria di un'applicazione potrebbe essere implementata come un nuovo processo a thread singolo piuttosto che come un processo a più thread.\par
Ci sono almeno 2 motivi per cui questa non è la scelta migliore:
\begin{itemize}
    \item La \textbf{comunicazione} tra thread è significativamente più veloce di quella tra processi 
    \item Il \textbf{context switch} tra thread è molto più veloce che tra processi
\end{itemize}


\subsubsection{Vantaggi}
Possiamo scrivere di quattro importanti vantaggi:
\begin{itemize}
    \item \textbf{Reattività}, poiché un processo potrebbe fornire una risposta in modo rapido mentre tutti gli altri thread sono bloccati o rallentati a causa dell'intenso uso della CPU
    \item \textbf{Condivisione} delle risorse "globali" come codice, dati e spazio di indirizzamento
    \item \textbf{Economicità}, poiché creare e gestire thread è più veloce rispetto alla creazione e gestione dei processi
    \item \textbf{Scalabilità}, poiché un processo single-thread può essere eseguito su un singolo core, mentre ogni thread di un processo multithread può essere suddiviso tra tutti i core disponibili della CPU (solo per architetture multi-core)
\end{itemize}

\subsubsection{Programmazione Multi-core}
Una tendenza recente nell'architettura dei computer è quella di produrre chip con più core, o CPU su un singolo chip. Un'applicazione multi-thread in esecuzione su un tradizionale chip single-core dovrebbe intercalare i thread. Su un chip multi-core, tuttavia, i thread potrebbero essere distribuiti tra i core disponibili, consentendo una vera \textbf{elaborazione parallela}. Si parla di \textbf{concorrenza} infatti quandono vengono eseguiti processi multi-threaded su CPU single core.\footnote{Le CPU sono state sviluppate per supportare più thread simultanei per core nell'hardware (ad esempio, l'hyper-threading di Intel). Ogni core fisico appare come due processori al sistema operativo, consentendo la pianificazione simultanea di due processi per core}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.55\textwidth,keepaspectratio]{{im/core}}
        \caption{I chip multi-core richiedono nuovi algoritmi di pianificazione del sistema operativo per utilizzare al meglio i più core disponibili}    
    \end{center}
\end{figure}

\subsubsection{Tipi di parallelismo}
In teoria, ci sono due modi per parallelizzare il carico di lavoro:
\begin{itemize}
    \item Parallelismo dei \textbf{dati}: suddivide i dati tra più core (thread) e
    esegue la stessa attività su ciascun blocco di dati
    \item Parallelismo dei \textbf{compiti}: suddivide i diversi compiti da eseguire tra i diversi core e li esegue contemporaneamente
    \item In pratica, nessun programma è mai suddiviso unicamente dall'uno o dall'altro di questi, ma piuttosto da una sorta di combinazione \textbf{ibrida}
\end{itemize}

\subsubsection*{Classificazione degli OSs}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.55\textwidth,keepaspectratio]{{im/os}}
        \caption{}    
    \end{center}
\end{figure}

\subsubsection{Kernel threads vs User threads}
Il supporto per (più) thread può essere fornito in due modi ossia a livello di \textbf{kernel} (thread del kernel) e a livello \textbf{utente} (thread utente). \\
I thread del kernel viene gestito direttamente dal kernel del sistema operativo stesso, mentre i thread utente vengono gestiti in spazio utente da una libreria di thread a livello utente, senza intervento del sistema operativo.\par
I \textbf{thread del kernel} hanno la più piccola unità di esecuzione che può essere pianificata dal sistema operativo. Il sistema operativo è responsabile del supporto e della gestione di tutti i thread. Possiedono un Process Control Block (\textbf{PCB}) per ogni processo, un Thread Control Block (\textbf{TCB}) per ogni thread. Il sistema operativo di solito fornisce chiamate di sistema per creare e gestire i thread dallo spazio utente.\par
Tra i \textbf{vantaggi} del Kernel Thread:
\begin{itemize}
    \item Il kernel ha piena conoscenza di tutti i thread
    \item Lo Scheduler può decidere di concedere più tempo di CPU a un processo con un gran numero di thread
    \item Buono per le applicazioni che si bloccano frequentemente
    \item Il passaggio da un thread all'altro è più rapido del passaggio da un processo all'altro
\end{itemize}
Tra gli \textbf{svantaggi}:
\begin{itemize}
    \item Overhead significativo e aumento del kernel
    complessità
    \item Lento e inefficiente (necessita di invocazioni del kernel)
    \item Il Context switching, sebbene più leggero, è gestito dal kernel
\end{itemize}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/kt}}
        \caption{}    
    \end{center}
\end{figure}

Gli \textbf{User Thread} invece vengono gestiti interamente dal sistema run-time (libreria thread a livello utente). Il kernel del sistema operativo non sa nulla dei thread a livello utente e gestisce i thread a livello utente come se fossero processi a thread singolo. Idealmente, le operazioni sui thread dovrebbero essere veloci quanto una chiamata di funzione.\par
Tra i \textbf{vantaggi}:
\begin{itemize}
    \item Veramente veloce e leggero
    \item Le politiche di programmazione sono più flessibili
    \item Può essere implementato in sistemi operativi che non supportano il threading
    \item Nessuna chiamata di sistema coinvolta, solo chiamate di funzione in spazio utente
    \item Nessun Context Switch effettivo
\end{itemize}
\pagebreak
Tra gli \textbf{svantaggi}:
\begin{itemize}
    \item Nessuna vera concorrenza di multi-thread
    processi
    \item Decisioni di pianificazione sbagliate
    \item Mancanza di coordinamento tra kernel e thread
    \item Un processo con 100 thread compete per un intervallo di tempo con un processo con solo 1 thread
    \item Richiede chiamate di sistema non bloccanti, altrimenti tutti i thread all'interno di un processo devono attendere
\end{itemize}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/ut}}
        \caption{}    
    \end{center}
\end{figure}

Un \textbf{lightweight process (LWP)} è un processore virtuale attivo nella user space contenente un singolo kernel thread, mentre multipli user thread gestiti tramite una libreria per i thread vengono posti su uno o più LWP, i quali assumono un ruolo di "tramite" tra le due tipologie di thread.

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/lwp}}
        \caption{Hybrid Management: Lightweight Processes}    
    \end{center}
\end{figure}
\pagebreak
\subsubsection{Modelli di Multi-threading}
In un'implementazione specifica, i thread utente devono essere mappati ai thread del kernel e ne esistono di quattro tipi:\footnote{Un thread del kernel è l'unità di esecuzione pianificata dal sistema operativo per l'esecuzione sulla CPU (simile al processo a thread singolo)}
\begin{itemize}
    \item Molti a uno (puro livello utente)
        \begin{itemize}
            \item Molti thread utente sono tutti mappati su un singolo thread del kernel
            \item Il processo può eseguire un solo thread utente alla volta perché
            c'è solo un thread del kernel ad esso associato
            \item Poiché un singolo thread del kernel può operare su una singola CPU, i processi multi-user-thread non possono essere suddivisi su più CPU
            \item Se viene effettuata una chiamata di sistema bloccante, l'intero processo si blocca, anche se altri thread utente sarebbero in grado di continuare
        \end{itemize}
    \item Uno a uno (puro livello kernel)
        \begin{itemize}
            \item Un thread del kernel separato per gestire ciascun thread utente
            \item Supera i limiti del blocco delle chiamate di sistema
            e suddivisione dei processi su più CPU
            \item Il sovraccarico della gestione del modello uno a uno è più significativo e può rallentare il sistema
            \item La maggior parte delle implementazioni di questo modello pone un limite al numero di thread che possono essere creati
        \end{itemize}
    \item Molti a molti 
        \begin{itemize}
            \item Esegue il multiplexing di un numero qualsiasi di thread utente su un numero uguale o inferiore di thread del kernel
            \item Gli utenti non hanno restrizioni sul numero di thread creati
            \item I processi possono essere suddivisi su più processori
            \item Il blocco delle chiamate di sistema del kernel non blocca l'intero processo
        \end{itemize}
    \item Due livelli
        \begin{itemize}
            \item Una variante del modello molti-a-molti
            \item Combina molti a molti con uno a uno
            \item Aumenta la flessibilità delle politiche di programmazione
        \end{itemize}
\end{itemize}

\pagebreak

\begin{figure}[h]
    \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\linewidth]{im/1m}
    \caption{Molti a uno}    
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\linewidth]{im/2m}
    \caption{Uno a uno}    
    \end{minipage}
    \end{figure}
    \begin{figure}[h]
    \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\linewidth]{im/3m}
    \caption{Molti a molti}    
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\linewidth]{im/4m}
    \caption{Due livelli}    
    \end{minipage}
    \end{figure}

    Quando un thread effettua una syscall \texttt{fork() {o} exec()} in un processo, si presenta la questione di come gestire la creazione di un nuovo processo. In particolare, ci sono due opzioni:
    \begin{enumerate}
        \item Copiare l'intero processo, comprensivo di tutti i thread presenti, al fine di generare un nuovo processo con una copia esatta dell'originale;
        \item Generare un nuovo processo single-thread contenente una copia del singolo thread che ha effettuato la syscall.
    \end{enumerate}
        
    La scelta tra queste due opzioni dipende dalla progettazione dell'OS, che deve garantire il corretto funzionamento del sistema. Tuttavia, in molti casi, è possibile adottare una \textbf{soluzione ibrida}: se il nuovo processo viene eseguito subito dopo la syscall, allora non c'è necessità di copiare anche gli altri thread del processo originale. Invece, se il nuovo processo viene eseguito in un momento successivo, l'intero processo dovrebbe essere copiato.
    
    Nel caso in cui un processo multi-thread riceva un segnale, la gestione di quale thread è il destinatario del segnale è un'importante problematica da affrontare. Esistono diversi approcci per gestire questa problematica, tra cui:
    \begin{enumerate}
        \item Invio del segnale al thread specifico che ne ha bisogno.
        \item Invio del segnale a tutti i thread del processo.
        \item Invio del segnale solo ad alcuni thread specifici del processo.
        \item Scelta di un thread specifico che gestisca tutti i segnali ricevuti.
    \end{enumerate}
\pagebreak
    Perché un thread possa essere schedulato ed eseguito dalla CPU, deve competere con gli altri thread che cercano di accedere alla stessa risorsa. La gestione dei thread contendenti avviene tramite il contention scope, che può essere di due tipologie: \textbf{Process Contention Scope (PCS) {e} System Contention Scope (SCS)}.\\

    Nel \textbf{PCS}, la competizione avviene tra i thread appartenenti allo stesso processo. Per implementare questa tipologia di contention scope, il sistema deve utilizzare un modello Molti a Molti o Molti a Uno.\par Nel primo caso, i thread sono mappati su un insieme di kernel threads e il sistema operativo si occupa di distribuire i kernel threads sui processori disponibili. Nel secondo caso, i thread sono mappati su un singolo kernel thread e la distribuzione sui processori è gestita dal sistema operativo.\\
    
    Nello \textbf{SCS}, la competizione avviene tra tutti i thread di ogni processo e viene implementato in sistemi basati sul modello One-to-One, dove ogni thread è mappato su un kernel thread dedicato.\\
    
    Per comunicare con le librerie per i thread nel momento in cui un evento si verifica, il kernel utilizza un lightweight process (\textbf{LWP}). Le up-call, ovvero le chiamate dal kernel alla libreria, vengono gestite da un upcall handler presente nella libreria stessa. Ogni upcall fornisce un nuovo LWP tramite cui l'upcall handler verrà eseguito, garantendo una gestione efficiente e ottimizzata dei thread.
\pagebreak

\section{Sincronizzazione tra Processi/Thread}

Abbiamo già accennato al fatto che i processi/thread possono cooperare tra loro per raggiungere un compito comune. Tuttavia, la cooperazione può richiedere la \textbf{sincronizzazione}\footnote{La sincronizzazione come soluzione al problema della sezione critica} tra i thread a causa della presenza delle cosiddette sezioni critiche (o regioni critiche). Le \textbf{primitive} di sincronizzazione sono necessarie per garantire che solo un thread alla volta esegua una sezione critica.\par Le \textbf{race condition} sono situazioni dove due o più processi stanno scrivendo o leggendo i medisimi dati condivisi e il risultato finale dipende da chi viene prescelto per l'esecuzione.\footnote{La \textbf{mutua esclusione} si occupa di bloccare un processo che deve leggere dei dati che vengono già letti da un altro processo }
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/cs}}
        \caption{The Critical Section Problem}    
    \end{center}
\end{figure}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.3\textwidth,keepaspectratio]{{im/cs1}}
        \caption{The Critical Section Problem}    
    \end{center}
\end{figure}


Qualsiasi soluzione di sincronizzazione al problema della sezione critica deve soddisfare tre proprietà:
\begin{itemize}
    \item \textbf{Mutua esclusione}, ossia solo un processo/thread alla volta può trovarsi nella sua sezione critica
    \item  \textbf{Liveness}, ossia se nessun processo è nella sua sezione critica, e uno o più vogliono eseguirlo, allora ognuno di questi deve essere in grado di entrare nella sua sezione critica
    \item  \textbf{Bounded Waiting}, ossia un processo che richiede l'ingresso nella sua sezione critica avrà un turno alla fine, e c'è un limite su quanti altri possono iniziare per primi.
\end{itemize}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/bob}}
        \caption{The Critical Section Problem}    
    \end{center}
\end{figure}


Nell'esempio del latte:
\begin{itemize}
    \item  Garantire l'esclusione reciproca significa non più latte di quanto sarà necessario
    comprare (cioè, solo uno tra \textit{Bob} e \textit{Carla} comprerà il latte se necessario)
    \item  Garantire la liveness significa che qualcuno dovrebbe comprare del latte (cioè, l'opzione dove sia \textit{Bob} che \textit{Carla} non fanno nulla è sicuramente sicuro ma indesiderabile)
    \item  Garantire l'attesa del limite significa che alla fine \textit{Bob} e \textit{Carla} entreranno nella loro sezione critica
\end{itemize}
    

Per poter sincronizzare i processi/thread tra di loro, i linguaggi di programmazione forniscono delle primitive atomiche basate su una delle seguenti tre soluzioni:
\begin{itemize}
    \item La sincronizzazione avviene tramite un \textbf{lock}, dove prima di accedere ad un settore critico un processo acquisisce tale lock, per poi rilasciarlo una volta uscito dal settore critico
    \item La sincronizzazione avviene tramite un \textbf{semaforo}, una generalizzazione del lock
    \item La sincronizzazione avviene tramite un \textbf{monitor}, il quale connette dei dati condivisi alle primitive di sincronizzazione
\end{itemize}


\subsection{I Lock}
Fornire l'esclusione reciproca ai dati condivisi utilizzando due primitive atomiche. L'acquisizione di una lick deve avvenire in modo atomico, evidanto che lo scheduler interrompa l'acquisizione: 
\begin{itemize}
    \item \texttt{Lock.acquire()} à attendere che il blocco sia libero, quindi acquisirlo
    \item \texttt{Lock.release()} per sbloccare e riattivare qualsiasi thread di attesa in \texttt{acquire()}
\end{itemize}

Esistono delle regole per l'utilizzo di un lock:
\begin{itemize}
    \item \textbf{Acquisire} sempre il blocco prima di accedere ai dati condivisi
    \item \textbf{Rilasciare} sempre il blocco dopo aver terminato con i dati condivisi 
    \item Il blocco deve essere inizialmente \textbf{libero}
    \item Solo un processo/thread può acquisire il lock, gli altri \textbf{aspetteranno}
\end{itemize}

Poiché lo scheduler della CPU prende il controllo solo a seguito di eventi interni, ossia quando il thread in esecuzione lascia il controllo della CPU, oppure a seguito di eventi esterni, per evitare problemi all'interno dei settori critici è necessario impedire lo scheduling durante il rilascio o l'acquisizione di un lock. Per implementare le \textbf{primitive} ad alto livello relative alla sincronizzazione, quindi, è necessario del supporto hardware di basso livello che possa eseguire tali \textbf{operazioni atomiche}\footnote{Un'operazione viene detta atomica se essa non può essere interrotta in alcun modo, impedendo quindi che possa avvenire un context switch durante la sua esecuzione.}, in particolare tramite la \textbf{disabilitazione degli interrupt}\footnote{La prima soluzione a supporto hardware per poter implementare la sincronizzazione, corrisponde alla disabilitazione degli interrupt mentre un thread acquisisce o rilascia un lock} o l'uso di istruzioni atomiche.


\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.3\textwidth,keepaspectratio]{{im/lock}}
        \caption{The Critical Section Problem}    
    \end{center}
\end{figure}

Sui sistemi a CPU singola, possiamo impedire che lo scheduler possa prendere il sopravvento: 
\begin{itemize}
    \item eventi \textbf{interni} imponendo ai thread di non richiedere alcuna operazione di I/O durante un
    sezione critica
    \item evento \textbf{esterno} per disabilitare gli interrupt (ovvero, dire all'HW di ritardare la gestione di qualsiasi evento esterno fino a quando il thread corrente non è terminato con la sezione critica)
\end{itemize}
Copriamo tutti i possibili casi in cui il thread corrente potrebbe perdere il controllo della CPU, volontariamente (a causa di eventi interni) o involontariamente (a causa di eventi esterni).\par
In sintesi supponiamo di avere una sola variabile condivisa (lock), inizialmente a 0. Quando un processo vuole entrare nella sua regione critica, prima controlla il lock. Se è 0, il processo lo imposta a 1 ed entra nella regione critica. Se è già a 1, il processo aspetta finché non vale 0. In questo modo 0 significa che nessun processo è nella sua regione critica, 1 che un qualche processo è nella sua regione critica.\par
Sfortunatamente questa idea presenta esattamente lo stesso \textbf{difetto} constatato nella directory di pool di stampa. Supponiamo che un processo legga il lock e veda che è 0. Prima che riesca a metterlo a 1, viene eseguito un altro processo schedulato e imposta il lock a 1.\par
Quando riparte ancora il primo processo, anch'esso lo imposterà a uno e i due processi sarebbero nella loro regione critica nello stesso momento.

\subsubsection*{Istruzione atomica}
Un'istruzione atomica di lettura-modifica-scrittura legge un valore dalla memoria in un registro e scrive un nuovo valore in un colpo solo. Su un uniprocessore sarà semplice implementare l'aggiunta di una nuova istruzione mentre su un multiprocessore, anche il processore che impartisce l'istruzione deve essere in grado di farlo invalidare qualsiasi copia del valore che altri processi potrebbero avere nella loro cache.\par
Prendiamo in esempio il \texttt{test\&set} (la maggior parte delle architetture) legge un valore, riscrive in memoria oppure uno \texttt{scambio(x86)} scambia valori tra registro e memoria.

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/testset}}
        \caption{The Critical Section Problem}    
    \end{center}
\end{figure}
L'implementazione del lock tramite \textbf{disabilitazione degli interrupt} presenta alcune
\textbf{problematiche}, infatti ha la necessità dell'invocazione del kernel che rende il tutto più complesso. Inoltre risulta inutilizzabile in sistemi multi-core.\par
Esistono due \textbf{problemi} principali con le istruzioni atomiche: il \textbf{busy waiting}\footnote{L'espressione busy waiting o busy wait (letteralmente "attesa impegnata", più spesso tradotto come "attesa attiva") indica una tecnica di sincronizzazione per cui un processo o un thread che debba attendere il verificarsi di una certa condizione (per esempio la disponibilità di input dalla tastiera o di un messaggio proveniente da un altro processo) lo faccia verificando ripetutamente (ciclicamente) tale condizione. Questo approccio è alternativo all'uso di una sospensione del processo e del suo successivo risveglio tramite un segnale specifico (per esempio un interrupt nel caso dell'input da tastiera). Fonte: Wikipedia\par} (che andrebbe generalmente evitato dato che consuma tempo di CPU)\footnote{un lock che utilizza il busy waiting si chiama \textbf{spin lock}}.\par
Infine abbiamo la \textbf{unfairness} in quanto non esiste una coda in cui i thread attendono il rilascio del blocco.\\

Nonostante il tempo in attesa della CPU sia un problema irrisolvibile nel caso delle istruzioni atomiche, esso può essere minimizzato. Inoltre, l'aggiunta di una queue permette di gestire in modo deterministico quale thread vada ad ottenere il lock a seguito del suo rilascio:

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.4\textwidth,keepaspectratio]{{im/lock2}}
        \caption{The Critical Section Problem}    
    \end{center}
\end{figure}
Esiste un miglioramento di \texttt{test\&set} per ridurre l'attesa occupata rendendola indipendente da quanto tempo è delimitata la sezione critica da acquisizione e rilascio:

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.65\textwidth,keepaspectratio]{{im/bw}}
        \caption{The Critical Section Problem}    
    \end{center}
\end{figure}

\pagebreak
\subsection{I Semafori}
I semafori sono un'altra struttura dati che fornisce mutua esclusione alle sezioni critiche. Può inoltre svolgere il ruolo di contatore atomico.\par
Tipo speciale di variabile (intera) che supporta 2 operazioni atomiche
\begin{itemize}
    \item \texttt{wait()} (anche \texttt{P()}): decremento, blocco fino all'apertura del semaforo
    \item \texttt{signal()} (anche \texttt{V()}): incremento, consente l'ingresso di un altro thread
\end{itemize}
A ciascun semaforo è associata una queue di processi/thread in attesa. 
Quando \texttt{wait()} viene chiamato da un thread, se il semaforo è aperto il thread continua, altrimenti il thread si blocca in coda. Successivamente \texttt{signal()} apre il semaforo. Se un thread è in attesa in coda il thread viene sbloccato, mentre se non ci sono thread in attesa in coda, il segnale viene ricordato per il thread successivo. In altre parole, \texttt{signal()} è stateful e ha una "history". \\


Esistono due categorie di semafori:
\begin{itemize}
    \item \textbf{Binary Semaphore} detti Mutex\footnote{In sostanza i Mutex sono utili solo per gestire la mutua esclusione di alcune risorse condivise o di pezzi di codice} (lo stesso di un Lock), hanno il compito di garantire l'accesso mutuamente esclusivo a una risorsa (ad esempio, solo un processo/thread esegue in una sezione critica). Ad essi vengono assegnati la variabile intera associata può assumere solo 2 valori: 0 o 1. Viene inizializzato per aprire (ad esempio, valore = 1).
    \item \textbf{Conteggio Semaforo}, utile per gestire più risorse condivise. Il valore iniziale del semaforo è solitamente il numero di risorse. Un processo può accedere a una risorsa purché ne sia disponibile almeno una.
\end{itemize}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/sema1}}
        \caption{Ogni semaforo supporta una coda di processi in attesa di accedere alla sezione critica (es. per acquistare latte)}    
    \end{center}
\end{figure}
Se un processo esegue \texttt{S.wait()} e il semaforo S è aperto (diverso da zero), continua l'esecuzione, altrimenti il sistema operativo mette il processo in coda di attesa. Un \texttt{S.signal()} sblocca un processo sulla coda di attesa del semaforo S. \par
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/sema2}}
        \caption{Esempio semaforo binario}    
    \end{center}
\end{figure}
\pagebreak
Vediamo ora l'implementazione del semaforo:
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/sema3}}
        \caption{\texttt{wait() {e} signal()} sono istruzioni atomiche}    
    \end{center}
\end{figure}

\subsubsection{Esempi}
\begin{enumerate}
    \item Considerando i seguenti due processi A e B, un possibile sviluppo dell'esecuzione è il seguente:
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/sema4}}
            \caption{}    
        \end{center}
    \end{figure}
    \item Consideriamo i due seguenti processi Producer e Consumer:
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.45\textwidth,keepaspectratio]{{im/sema5}}
            \caption{}    
        \end{center}
    \end{figure}
    \pagebreak
    \begin{itemize}
        \item I due processi condividono un buffer comune. La variabile counter tiene traccia del numero di elementi attualmente nel buffer.
        \item In tale caso, potrebbe crearsi una race condition: in assenza di sincronizza- zione, i due processi potrebbero lavorare contemporaneamente sulla variabile
        \item Ad esempio, ipotiziamo che inizialmente si abbia counter = 5. In tal caso, una possibile esecuzione del programma potrebbe essere la seguente:
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/sema6}}
                \caption{}    
            \end{center}
        \end{figure}
    \end{itemize}
\end{enumerate}

\subsubsection{Problemi semafori}
Non è facile ottenere il significato di attesa/segnalazione su un semaforo, infatti si tratta essenzialmente di variabili globali condivise. Non esiste inoltre una connessione diretta tra il semaforo ei dati ai quali il semaforo controlla l'accesso. Servono a molteplici scopi (ad esempio, mutex, vincoli di pianificazione, ecc\dots).\par
La loro correttezza dipende dall'abilità del programmatore.\\
I semafori possono possono creare deadlock, ossia una situazione in cui due o più processi o azioni si bloccano a vicenda, aspettando che uno esegua una certa azione (es. rilasciare il controllo su una risorsa come un file, una porta input/output ecc.) che serve all'altro e viceversa.



\pagebreak

\subsection{I Monitor}
Un \textbf{monitor} è un costrutto del linguaggio di programmazione che controlla l'accesso ai dati condivisi. E' simile a una classe (Java/C++) che incorpora tutto insieme: dati, operazioni e sincronizzazione. Il codice di sincronizzazione viene aggiunto dal compilatore, applicato in fase di esecuzione. Diversamente dalle classi, i monitor garantiscono la mutua esclusione, cioè solo un thread alla volta può eseguire il metodo di un monitor, inoltre richiedono che tutti i dati siano privati.\\

I monitor definiscono un blocco e zero o più variabili di condizione per la gestione dell'accesso simultaneo ai dati condivisi. Utilizza il blocco per garantire che all'interno del monitor sia attivo un solo thread alla volta. Il blocco prevede, come anticipato prima, l'esclusione reciproca per i dati condivisi.
\\

È semplice trasformare una classe Java in un monitor semplicemente, basta rendendere privati tutti i dati, sincronizzare tutti i metodi (o quelli non privati). 

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/monitor1}}
        \caption{La parola chiave \textit{synchronized} indica che il metodo è soggetto a mutua esclusione. }    
    \end{center}
\end{figure}

Nell'esempio precedente, il metodo \texttt{remove()} deve attendere finché qualcosa non è disponibile nella coda. Intuitivamente, il thread dovrebbe dormire all'interno della sezione critica. Ma se il thread dorme mentre mantiene ancora un blocco, nessun altro thread può accedervi alla coda, aggiungi un elemento ad essa e alla fine riattiva il thread dormiente. Le variabili condizionali sono delle soluzioni che risolvono questo problema. Consentono a un thread di dormire all'interno di una sezione critica. Qualsiasi blocco mantenuto dal thread viene rilasciato atomicamente prima di andare a dormire. \par

Ogni variabile\footnote{Nota: le variabili di condizione non sono oggetti booleani} di condizione supporta tre operazioni:
\begin{itemize} 
    \item \texttt{wait} release lock e vai a dormire atomicamente (coda di camerieri)
    \item \texttt{signal} sveglia un thread in attesa se ne esiste uno, altrimenti non fa nulla \item \texttt{broadcast} risveglia tutti i thread in attesa
\end{itemize}
Il thread deve mantenere il blocco durante l'esecuzione di operazioni sulle variabili di condizione.\par
Le variabili di condizione hanno le stesse operazioni dei semafori ma semantica completamente diversa:
\begin{itemize}
    \item L'accesso al monitor è controllato da una serratura
    \item wait() blocca il thread chiamante e rinuncia al blocco
    \item per chiamare wait(), il thread deve essere nel monitor (quindi, ha il blocco) 
    \item su un semaforo, wait() blocca solo il thread sulla coda
    \item signal() provoca l'attivazione di un thread in attesa 
    \item Se non c'è un thread in attesa, il segnale viene comunque perso!
    \item su un semaforo, il segnale aumenta il contatore, consentendo l'ingresso futuro anche se nessun thread è attualmente in attesa
\end{itemize}
Comunemente, i monitor vengono implementati secondo due stili:
\begin{itemize}
    \item Lo stile Mesa, dove il thread segnalante inserisce un thread in attesa nella ready queue mentre il primo continua l'esecuzione all'interno del monitor, e dove la condizione, la quale deve essere verificata continuamente, non deve essere necessariamente verificata quando il thread in attesa viene nuovamente eseguito
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/mesa}}
            \caption{}    
        \end{center}
    \end{figure}

    \item Lo stile Hoare, dove il thread segnalante viene trasformato immediatamente in un thread in attesa e dove la condizione anticipata dal thread in attesa è garantita quando esso viene eseguito
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/hoare}}
            \caption{}    
        \end{center}
    \end{figure}
\end{itemize}

\pagebreak
\subsubsection{Problemi di Lettura e Scrittura}
Un oggetto è condiviso tra più thread, ciascuno appartenente a una delle due classi: 
\begin{itemize}
    \item Reader: legge i dati, non li modifica mai
    \item Scrittori: leggono i dati e li modificano
\end{itemize}

La soluzione più semplice suggirisce di utilizzare un singolo blocco sull'oggetto dati per ciascuna operazione, ma potrebbe risultare troppo restrittivo.\par
Esistono due varianti del problema a seconda che la priorità sia sui lettori o sugli scrittori:
\begin{itemize}
    \item primo problema di lettori-scrittori (priorità ai lettori), se un lettore desidera accedere ai dati e non vi è già un utente che vi accede, allora l'accesso è consentito al lettore. Esiste anche una possibile "fame" degli scrittori, in quanto potrebbero esserci sempre più lettori in arrivo per accedere ai dati
    \item secondo problema lettori-scrittori (priorità agli scrittori), si verifiva quando un writer vuole accedere ai dati, salta in testa alla coda. Esiste anche una possibile "fame" dei lettori, in quanto sono tutti bloccati finché ci sono scrittori.
\end{itemize}
\subsubsection{(First) Readers-Writers Problem: Soluzione I}
E' possibile utilizzare un contatore e due semafori binari:
\begin{itemize}
    \item \texttt{numReaders}, utilizzato dai processi reader per contare il numero di lettori
    attualmente accedendo ai dati
    \item semaforo \texttt{mutex} binario, utilizzato solo dai lettori per l'accesso controllato
    numReader
    \item \texttt{rw\_mutex}, binario semaforo utilizzato per bloccare e rilasciare i writer
\end{itemize}

Il primo lettore che arriva bloccherà su \texttt{rw\_mutex} se c'è attualmente un utente che sta accedendo ai dati. Tutti i lettori successivi si bloccheranno solo sul \texttt{mutex} per il loro turno di incremento \texttt{numReader}.\par
Il primo lettore si blocca se è presente un writer; qualsiasi altro lettore che tenta di entrare viene bloccato su \texttt{mutex}. Solo l'ultimo lettore ad uscire segnala uno scrittore in attesa. Quando uno scrittore esce, se c'è sia un lettore che uno scrittore in attesa, dipende da chi va dopo sul programmatore.\par Se uno scrittore esce e un lettore va dopo, allora tutti i lettori che sono in attesa falliranno (almeno uno è in attesa su \texttt{rw\_mutex} e zero o più possono essere in attesa su \texttt{mutex}). In alternativa, lascia che uno scrittore entri prima nella sua sezione critica (priorità per gli scrittori).


\subsubsection{Soluzione II: usare i Monitor}
\pagebreak
\subsection{Deadlock}

Consideriamo il famoso problema dei filosofi a cena:
\begin{itemize} 
    \item Ci sono 5 filosofi seduti ad un tavolo rotondo
    \item Ogni filosofo ha una bacchetta di legno alla propria sinistra, per un totale di 5 bacchette
    \item Per poter mangiare, ogni filosofo necessita di due bacchette
    \item L'obiettivo è far sì che ogni filosofo mangi
\end{itemize}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/filo}}
        \caption{}    
    \end{center}
\end{figure}
\begin{itemize}
    \item Esiste una prima soluzione banale al problema: possiamo utilizzare un lock globale che permette ad un singolo filosofo per volta di poter prendere in mano due bacchette. Tale soluzione, risulta essere inefficiente per via dell'assenza di concorrenza tra i filosofi affinché essi possano mangiare contemporaneamente
    \item Una seconda soluzione prevede l'uso di un semaforo:
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/filo1}}
                \caption{}    
            \end{center}
        \end{figure}
    \item Tale soluzione presenta alcune problematiche: nel caso in cui ogni filosofo prenda in mano la bacchetta sinistra in contemporaneamente agli altri, si andrebbe a creare un \textbf{deadlock}, dove ogni filosofo rimarrebbe in attesa che la bacchetta destra sia libera.
    \item Per far si che non si crei un deadlock e che vi sia concorrenza tra i filosofi, una terza soluzione prevede l'uso dei \textbf{monitor}: prima di prendere in mano una delle due bacchette, è necessario assicurarsi che la seconda sia libera, altrimenti sarà necessario aspettare che entrambe siano libere. Dunque, sarà necessario controllare se il filosofo alla propria destra e il filosofo alla propria sinistra stiano mangiano oppure no (variabili condizionate), portando nessun filosofo a prender in mano una singola bacchetta.
\end{itemize}
I problemi che abbiamo visto finora sono interessanti perché identificano alcuni modelli che sono molto comuni nella pratica: \textbf{Produttore-Consumatore},  \textbf{Lettore audio/video} incorporato in un browser Web che contiene un buffer dati condiviso + rete e rendering, Lettore-Scrittore (E.g sistema bancario: read vs. update account balances), oppure i \textbf{Filosofi a tavola} che bloccano più risorse: (ad esempio, prenotazioni di viaggi (hotel, compagnie aeree, database di autonoleggio)). 

\subsubsection{Cos'è un deadlock}
Intuitivamente, una condizione in cui due o più thread sono in attesa di un evento che può essere generato solo dagli stessi thread. I deadlock possono verificarsi sia su risorse hardware sia su risorse software. Si può verificare quando più thread competono per un numero finito di risorse:
\begin{itemize} 
    \item Rilevamento deadlock: trova istanze di deadlock e tenta di ripristinarle
    \item Prevenzione deadlock (offline): impone restrizioni/regole su come scrivere
    programmi senza deadlock
    \item Evitamento dei deadlock (online): il supporto runtime controlla le richieste di risorse effettuate dai thread per evitare i deadlock
\end{itemize}
Il \textbf{deadlock} è diverso dalla \textbf{starvation} i termini correlati ma ognuno si riferisce a una situazione specifica. La starvation si verifica quando un thread attende a tempo indeterminato una risorsa, ma altri thread stanno effettivamente facendo progressi utilizzando quella risorsa. La \textbf{differenza principale} con deadlock è che il sistema non è completamente bloccato (starvation), mentre con il deadlock il sistema rimane completamente bloccato \\

Il deadlock può verificarsi se tutte e quattro le condizioni sottostanti sono soddisfatte:
\begin{itemize}
    \item \textbf{Mutua esclusione}: almeno un thread deve contenere una risorsa non condivisibile (solo
    un thread contiene la risorsa)
    \item \textbf{Hold and Wait} almeno un thread detiene una risorsa non condivisibile ed è in attesa che altre risorse diventino disponibili (un altro thread detiene le risorse)
    \item \textbf{No Preemption} un thread può rilasciare una risorsa solo volontariamente; né un altro thread né il sistema operativo possono forzarlo a rilasciare la risorsa
    \item \textbf{Circular Wait} un insieme di thread in attesa $t_{1}, ..., t_{n}$ dove $t_{i}$ è in attesa su $t_{(i+1)\%n}$
\end{itemize}

\subsubsection{Resource Allocation Graph (RAG)}
Definiamo un \textbf{grafo orientato} \texttt{G = (V, E)} dove:
\begin{itemize}
    \item V è l'insieme dei vertici che rappresentano sia le risorse {$r_{1}, ..., r_{m}$} che i thread {$t_{1}, ..., t_{n}$}
    \item E è l'insieme degli archi tra risorse e thread
\end{itemize}
I bordi possono essere di 2 tipi:
\begin{itemize}
        \item \textbf{Archi di richiesta} un edge diretto $(t_{i}, r_{j})$ indica che $t_{i}$ ha richiesto $r_{j}$, ma non ancora acquisita
        \item \textbf{Archi di assegnamento} un edge diretto $(r_{j}, t_{i})$ indica che il sistema operativo ha assegnato $r_{j}$ a $t_{i}$
\end{itemize}
    

\subsubsection{Modellazione dei deadlock}
Holt (1972) mostrò come modellare queste quattro condizioni usando i grafi orientati i grafi hanno due tipi di nodi: 
\begin{itemize}
    \item i \textbf{processi}, mostrati come \textbf{cerchi}
    \item le \textbf{risorse}, mostrate come \textbf{quadrati}
\end{itemize}
Un \textbf{arco}, con una \textbf{freccia} a indicarne la direzione. Da un nodo risorsa (un quadrato) a un nodo processo (un cerchio) significa che la risorsa è stata in precedenza richiesta, assegnata e attualmente posseduta da quel processo.

\pagebreak
\subsubsection{Esempio}
\begin{enumerate}
    \item Prendiamo il seguente esempio:
    \begin{itemize}
        \item Consideriamo il seguente RAG:
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/rag}}
                \caption{}    
            \end{center}
        \end{figure}
        \item E' facilmente individuabile la presenza di un deadlock:
        \begin{enumerate}
            \item Il thread $t_{2}$, possedente la risorsa $r_{3}$, richiede la risorsa $r_{1}$, rimanendo in attesa che essa sia liberata dal thread $t_{3}$
            \item A questo punto, il thread $t_{3}$, possedente la risorsa $r_{1}$, richiede la risorsa $r_{2}$, rimanendo in attesa che essa sia liberata dal thread $t_{4}$
            \item A sua volta, il thread $t_{4}$, possedente la risorsa $r_{2}$, richiede la risorsa $r_{3}$, rimanendo in attesa che essa sia liberata dal thread $t_{2}$
            \item In definitiva, si è in una situazione dove $t_{2}$ aspetta $t_{3}$, il quale sta aspettando $t_{4}$, il quale a sua volta sta aspettando $t_{2}$, creando quindi un deadlock
        \end{enumerate}
        \item Se il thread $t_{4}$ non richiedesse la risorsa $r_{3}$, le condizioni necessarie affinché possa verificarsi un deadlock non sarebbero soddisfatte, per via dell'assenza dell'attesa circolare
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/rag2}}
                \caption{}    
            \end{center}
        \end{figure}
        \pagebreak
    \end{itemize}
    \item Prendiamo ora il seguente esempio:
    \begin{itemize}
        \item Consideriamo il caso in cui esistano due istanze della risorsa $r_{3}$, dove prima istanza è assegnata al thread $t_{2}$ e la seconda è assegnata al thread $t_{3}$.
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/rag3}}
                \caption{}    
            \end{center}
        \end{figure}
        \item Anche in tal caso, siamo in una situazione di deadlock:
        \begin{enumerate}
            \item $t_{4}$ richiede un'istanza di $r_{3}$, rimanendo in attesa che $t_{2}$ o $t_{3}$ ne rilascino almeno una
            \item Tuttavia, $t_{3}$ è in attesa che $t_{4}$ ceda $r_{2}$, mentre $t_{2}$ è in attesa che $t_{3}$ ceda $r_{1}$
        \end{enumerate}
    \end{itemize}
        \item Invece, se ci fossero tre istanze di $r_{3}$ e solo due di esse venissero utilizzate da $t_{2}$ e $t_{3}$, non si andrebbe a creare un deadlock, poiché $t_{4}$ non rimarrebbe in attesa, sbloccando quindi la catena
        \begin{figure}[hbt]
            \begin{center}
                \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/rag4}}
                \caption{}    
            \end{center}
        \end{figure}
\end{enumerate}

\subsection{Prevenire ed evitare un deadlock}

Bisogna scansionare il grafico di allocazione delle risorse (RAG) per i cicli e poi rompili.\par
    Esistono diversi modi per farlo:
    \begin{itemize}
        \item Uccidere tutti i thread nel ciclo ()
    \item Uccidere tutti i thread uno alla volta, forzando ciascuno di essi a rilasciare risorse 
    \item Anticipa le risorse una alla volta tornando a uno stato coerente (ad es. nelle transazioni di database)
\end{itemize}

Come visto nella sezione precedente, per prevenire un deadlock è sufficiente che una sola delle quattro condizioni necessarie non si verifichi.

Inoltre, è possibile studiare il comportamento di una sequenza di n thread per determinare se tale sequenza sia sicura o non:
\begin{itemize}
    \item Sia $m_{i}$ il numero massimo di risorse che il thread i possa richiedere
    \item Sia $c_{i}$ il numero di risorse attualmente occupate dal thread i
    \item Sia C = $\sum_{i=1}^{n} c_{i}$ il numero totale di risorse attualmente allocate nel sistema
    \item Sia R il numero massimo di risorse disponibili
    \item Definiamo una sequenza di thread come sicura se per ogni thread si verifica che:
    \begin{center}
        $m_{i} - c_{i} \leq R - C + \sum_{j=1}^{i-1} c_{j}$
    \end{center}
    \item dove:
    \begin{itemize}
        \item $m_{i} - c_{i}$ è il numero di risorse ancora richiedibili da $t_{i}$
        \item $R - C$ è il numero di risorse attualmente disponibili
        \item $\sum_{j=1}^{i-1} c_{j}$ è il numero di risorse attualmente allocate fino al thread $t_{j}$, dove $j < i$
    \end{itemize}
\end{itemize}

\subsubsection{Stato sicuro}

L'idea alla base della policy di allocazione delle risorse è quella di garantire la sicurezza del sistema attraverso la definizione di uno \textbf{stato sicuro}. Uno stato sicuro si verifica quando esiste una sequenza di esecuzione dei thread che permette a tutti i thread di terminare la loro esecuzione senza causare uno stato di interblocco. Tuttavia, la presenza di uno \textbf{stato insicuro} non implica necessariamente la presenza di un deadlock, poiché alcuni thread potrebbero non richiedere simultaneamente il massimo numero di risorse necessarie dichiarato.

Per garantire la sicurezza del sistema, la policy prevede che una risorsa venga assegnata ad un thread solo se, dopo l'allocazione, il sistema risulta in uno stato sicuro. In caso contrario, il thread viene messo in attesa della risorsa fino a quando non diventa disponibile in modo sicuro. Questa politica di gestione delle risorse previene la creazione di attese circolari, in cui i thread attendono risorse che non sono disponibili causando uno stato di interblocco del sistema.
\subsubsection{Archi di pretesa}
Oltre alla tipologia di archi di assegnamento che rappresentano l'allocazione effettiva di una risorsa ad un thread, viene utilizzata una variante del RAG che prevede anche archi di pretesa. Gli \textbf{archi di pretesa} indicano che un thread $t_i$ potrebbe richiedere in futuro la risorsa $r_{j}$, senza però richiederla immediatamente.

Nella gestione delle risorse, \textbf{soddisfare} una pretesa significa trasformare un arco di pretesa in un arco di assegnamento, cioè assegnare effettivamente la risorsa al thread che la richiede. La presenza di cicli nel grafo indica un possibile stato insicuro, in cui alcuni thread attendono risorse che altri thread detengono. Se l'assegnamento di una risorsa generasse uno stato insicuro, tale assegnamento non verrà effettuato anche nel caso in cui la risorsa sia effettivamente disponibile, in quanto potrebbe causare un interblocco del sistema. In questo modo, la gestione delle pretese aiuta a prevenire la creazione di situazioni di interblocco e a garantire la sicurezza del sistema.
\subsubsection{Esempi}
\begin{itemize}
    \item Consideriamo il seguente RAG:
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/rag21}}
            \caption{}    
        \end{center}
    \end{figure}
    \pagebreak
    \item Supponiamo che venga soddisfatta la pretesa del thread $t_{4}$. In tal caso, si andrebbe a creare uno stato insicuro poiché, nel caso in cui anche il thread $t_{3}$ vada a richiedere la risorsa $r_{3}$ (dunque trasformando l'arco di pretesa in un arco di richiesta) si andrebbe a creare un deadlock. Dunque tale pretesa non verrebbe soddisfatta.
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/rag22}}
            \caption{}    
        \end{center}
    \end{figure}
    \item Nel caso in cui invece venga soddisfatta la pretesa del thread $t_{2}$, si rimarrebbe in uno stato sicuro poiché, nel caso in cui il thread $t_{4}$ andasse a richiedere la risorsa $r_{3}$ esso rimarrebbe in attesa di $t_{3}$, il quale può continuare a lavorare poiché non è in attesa di nessuno. Dunque tale pretesa verrebbe soddisfatta.
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/rag23}}
            \caption{}    
        \end{center}
    \end{figure}

\end{itemize}

\pagebreak
\section{Gestione della Memoria}
La CPU recupera, decodifica ed esegue ripetutamente istruzioni da
memoria durante l'esecuzione di un programma. La maggior parte delle istruzioni implica il recupero dei dati dalla memoria o l'archiviazione dei dati in memoria, o entrambi. I chip di memoria rispondono solo agli \textbf{indirizzi fisici} effettivi. Risulta che i \textbf{nomi simbolici} utilizzati dai programmi utente devono essere eventualmente associati all'effettivo indirizzo di memoria fisica.

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/mem1}}
        \caption{}    
    \end{center}
\end{figure}

\begin{enumerate} 
    \item Fetch instruction at address 128
    \item Execute istruction: load from address [\%R2] (e.g., 1234)
    \item Fetch instruction at address 136
    \item Execute instruction: addition (no memory reference)
    \item Fetch instruction at address 144
    \item Execute instruction: store to address [\%R2] (1234)
\end{enumerate}

Un \textbf{nome simbolico} fa riferimento alla memoria simbolica utilizzato dai programmi utente. \textbf{L'indirizzo logico} è l'indirizzo di memoria generato dai programmi utente tramite la CPU. \textbf{L'indirizzo fisico} invece è la memoria effettiva su cui opera il chip di memoria. Il \textbf{binding di indirizzo} è la mappatura dall'indirizzo logico a quello fisico.\par
Ogni indirizzo logico viene \textbf{associato} ad un indirizzo fisico tramite tre modalità:
\begin{itemize}
    \item Compile time, ossia se la locazione fisica iniziale k dove un programma risiederà in memoria è nota al momento della compilazione (es.,k=0), il compilatore genera il cosiddetto codice assoluto. Non c'è l'intervento da parte del sistema operativo. Avremo perciò indirizzo fisico $==$ indirizzo logico. Se l'indirizzo di k varia il programma verrà ricompilato.
    \item Load time
    \item Execution time
\end{itemize}

\pagebreak
\subsubsection{Esempio}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/mem2}}
        \caption{Gli indirizzi logici non cambiano dopo che sono stati emessi come risultato della compilazione/assemblaggio}    
    \end{center}
\end{figure}

Se la locazione fisica iniziale k dove un programma risiederà in memoria non è nota al momento della compilazione il compilatore genera il cosiddetto codice rilocabile (staticamente), che fa riferimento a indirizzi relativi a k. Il caricatore del sistema operativo determina la posizione fisica iniziale di ogni processo k e avremo che  indirizzo fisico == indirizzo logico. Se il sistema operativo decide di utilizzare un diverso indirizzo di memoria fisica iniziale $k^{1}$ il programma deve essere ricaricato ma non compilato. 
\subsubsection{Esempio}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/mem3}}
        \caption{Il caricatore del sistema operativo aggiorna gli indirizzi logici prima di caricare l'eseguibile}
    \end{center}
\end{figure}

Se il programma può essere spostato nella memoria principale durante la sua esecuzione il compilatore genera il cosiddetto codice rilocabile (dinamicamente) o indirizzi virtuali. Il sistema operativo mappa gli indirizzi virtuali alle posizioni di memoria fisica utilizzando uno speciale supporto HW (MMU\footnote{Ha il compito di mappare gli indirizzi virtuali sugli indirizzi di memoria fisica }). In questo caso indirizzo fisico != indirizzo logico/virtuale. La soluzione più flessibile implementata dalla maggior parte dei sistemi operativi moderni.

\pagebreak
\subsubsection{Esempio}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/mem4}}
        \caption{Ogni indirizzo logico referenziato dalla CPU viene tradotto dall'HW in fase di esecuzione}
    \end{center}
\end{figure}
Esistono diverse tecniche di gestione della memoria in base al tipo di architettura che si vuole implementare. In particolare, si distinguono la gestione della memoria uni-programmabile e quella multi-programmabile.\par
Nella gestione della \textbf{memoria uni-programmabile}, all'OS viene assegnata una parte fissa di memoria fisica e viene eseguito un singolo processo per volta. Questo implica che il processo abbia sempre lo stesso indirizzo iniziale, consentendo l'associazione tra indirizzo logico e fisico in fase di compilazione.\par
Nella gestione della \textbf{memoria multi-programmabile}, invece, coesistono contemporaneamente più processi nella memoria, e i processi cooperanti condividono porzioni dello spazio di indirizzamento. Tuttavia, i processi non sono a conoscenza della condivisione della memoria e della porzione di memoria in cui si trovano, né sono in grado di accedere ai dati di altri processi e del sistema operativo, garantendo la sicurezza e l'integrità del sistema. Inoltre, le performance della CPU e della memoria non diminuiscono significativamente durante la condivisione e la frammentazione dei dati è limitata.
\pagebreak
\subsection{Rilocazione dei processi}
Si supponga che il sistema operativo sia allocato agli indirizzi di memoria più alti.Successivamente dobbiamo assumere che gli indirizzi logici generati da ciascun processo utente inizino da 0 in su all'indirizzo massimo (\texttt{memory\_size - os\_size - 1}). \par
Si carica un processo allocando il primo segmento contiguo di memoria in cui si adatta il processo. Questo consente la condivisione trasparente della memoria: lo spazio degli indirizzi di ogni processo può essere collocato ovunque nella memoria.\\
Nella \textbf{riallocazione statica} l'OS loader riscrive gli indirizzi generati da un processo, in modo da riflettere la sua posizione nella memoria principale (load time binding). Possiamo elencare alcuni vantaggi e svantaggi:
\begin{itemize}
    \item Vantaggi, ossia non è necessario alcun supporto HW.
    \item Svantaggi, ossia nessun processo di protezione/privacyà può danneggiare il sistema operativo o altri processi. Lo spazio degli indirizzi deve essere allocato in modo contiguo, presupponendo la richiesta dello stack e dell'heap nel caso peggiore. Il sistema operativo non può spostare un processo (spazio indirizzo) una volta allocato nella memoria.
\end{itemize}
    

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/mem5}}
        \caption{}
    \end{center}
\end{figure}

La \textbf{riallocazione dinamica} ha il compito di proteggere sistema operativo e processi gli uni dagli altri. Richiede supporto hardware (Memory Management Unit o MMU), associazione degli indirizzi in fase di esecuzione/esecuzione. MMU traduce dinamicamente ogni indirizzo logico/virtuale generato da un processo in un corrispondente indirizzo fisico.\par
MMU contiene almeno due registri, \textbf{base} che contiene la posizione di memoria fisica dello spazio degli indirizzi e \textbf{limit} che contiene dimensione limite dello spazio degli indirizzi.\par
La CPU supporta almeno due modalità operative:
\begin{itemize}
    \item modalità \textbf{privilegiata (kernel)} quando il sistema operativo è in esecuzione dopo aver emesso qualsiasi trap (chiamata di sistema, interruzione o eccezione), o quando si manipolano risorse sensibili (ad esempio, il contenuto dei registri MMU)
    \item modalità \textbf{utente} quando il processo utente è in esecuzione durante l'esecuzione delle istruzioni di processo sulla CPU
\end{itemize}
\pagebreak

La riallocazione dinamica presenta anch'essa dei vantaggi e svantaggi:
\begin{itemize}
    \item Tra gli svantaggi:
    \begin{itemize}
        \item Poco sovraccarico hardware da pagare ad ogni riferimento in memoria. Ogni processo deve ancora essere allocato in modo contiguo nella memoria fisica (possibile spreco di memoria).
        \item Il processo è ancora limitato dalla dimensione della memoria fisica.
        \item Il grado di multiprogrammazione è limitato poiché tutta la memoria di tutti i processi attivi deve entrare in memoria.
        \item Non esiste condivisione parziale dello spazio degli indirizzi (ad esempio, i processi non possono condividere il testo del programma).
    \end{itemize}
    \item Tra i vantaggi:
    \begin{itemize}
        \item Fornisce protezione (sia in lettura che in scrittura) tra gli spazi degli indirizzi 
        \item Il sistema operativo può facilmente spostare un processo durante l'esecuzione
        \item Il sistema operativo può consentire al processo di crescere dinamicamente nel tempo
        \item Implementazione hardware semplice e veloce (MMU):
        \item due registri speciali, un'operazione di aggiunta e una di confronto (possono essere eseguite in parallelo)
    \end{itemize}
\end{itemize}
Tra le \textbf{proprietà della riallocazione} troviamo:
\begin{itemize}
    \item I processi di \textbf{condivisione/trasparenza} non sono consapevoli della condivisione della memoria
    \item \textbf{Protezione/Sicurezza}: ogni riferimento di memoria è verificato in HW
    \item \textbf{Efficienza} in qualche modo raggiunta, ma se un processo cresce potrebbe essere necessario spostarlo in un'altra posizione (molto lento)
\end{itemize}
\pagebreak

\subsubsection{Policy di allocazione e frammentazione}
Finora abbiamo ipotizzato che ogni processo sia allocato in uno spazio contiguo di memoria fisica.Un metodo semplice consiste nel suddividere in anticipo tutta la memoria disponibile dedicata ai processi dell'utente in segmenti/partizioni di \textbf{uguali} dimensioni dove si assegnava ad ogni processo a un segmento. Questo però limita implicitamente il grado di multiprogrammazione (cioè il numero di processi simultanei) e la loro dimensione, per questo non viene più utilizzato.\par

Un approccio alternativo prevede che il sistema operativo tenga traccia dei segmenti di memoria \textbf{liberi} (non utilizzati), man mano che i processi entrano nel sistema, crescono e terminano. 


\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.7\textwidth,keepaspectratio]{{im/mem6}}
        \caption{L'OS deve essere in grado di scegliere correttamente lo spazio di memoria da riempire}
    \end{center}
\end{figure}

\subsubsection{Policy di allocazione della memoria: First-Fit}
Il criterio di allocazione della memoria "\textbf{First-Fit}" è una tecnica utilizzata nei sistemi operativi per allocare la memoria richiesta da un processo.\par

In pratica, il sistema operativo cerca nella memoria disponibile il primo blocco che abbia una dimensione \textbf{sufficiente} per soddisfare la richiesta del processo. Una volta trovato questo blocco, il processo viene allocato in esso e la memoria residua viene divisa in un nuovo blocco di dimensioni ridotte.\par

Questo approccio può portare ad una \textbf{frammentazione} della memoria, poiché potrebbero rimanere degli spazi vuoti tra i blocchi di memoria allocati. Tuttavia, il "First-Fit" è una tecnica semplice e rapida per allocare la memoria, ed è quindi utilizzata in molti sistemi operativi.\par
Complessità: \texttt{O(n)}, dove n è il numero di fori
\subsubsection{Esempio}
\begin{itemize}
    \item Supponiamo di avere tre buchi in memoria, rispettivamente di 20, 400 e 120 byte.
    \item Supponiamo che il processo X richieda 100 byte di memoria. Tramite la policy First-Fit, il processo verrebbe inserito nel primo buco sufficientemente grande, ossia quello da 400 byte, creando così un buco di 300 byte
    \item Supponiamo ora che il processo Y richieda 350 byte di memoria. L'OS non è in grado di soddisfare tale richiesta, poiché nessun buco è abbastanza grande.
\end{itemize}

\subsubsection{Policy di allocazione della memoria: Best-Fit}
Allocare il foro più piccolo sufficientemente grande per soddisfare la richiesta. Ciò consente di risparmiare grandi buchi per altre richieste di processo che potrebbero averne bisogno. Tuttavia, le parti inutilizzate risultanti dei fori potrebbero essere troppo piccole per essere utili e quindi andranno sprecate. La struttura dati che può essere utilizzata nel Best-Fit è il \textbf{Binary Search Tree (BST).}\par
Complessità: ancora \texttt{O(n)} ma può essere \texttt{O(log n)} se l'elenco dei fori viene mantenuto ordinato. 
\pagebreak
\subsubsection{Esempio}
\begin{itemize}
    \item Supponiamo di avere tre buchi in memoria, rispettivamente di 20, 400 e 120 byte.
    \item Supponiamo che il processo X richieda 100 byte di memoria. Tramite la policy best-fit, il processo verrebbe inserito nel buco minore sufficientemente grande, ossia quello da 120 byte, creando così un buco di 20 byte
    \item Supponiamo ora che il processo Y richieda 350 byte di memoria. Tale processo verrà allocato nel buco da 400 byte, creandone uno di 50.
    \item La somma dei buchi disponibili, quindi, corrisponde a 90 byte. Tuttavia, essendo suddivisi in buchi di piccole dimensioni, difficilmente un processo sarà abbastanza piccolo da poter essere contenuto in uno di tali buchi, portando ad uno spreco di memoria
\end{itemize}
    
\subsubsection{Policy di allocazione della memoria: Worst-Fit}

A differenza della policy Best-Fit, l'algoritmo Worst-Fit cerca di allocare il blocco di memoria libero \textbf{più grande} disponibile per soddisfare le richieste di memoria dei processi. In altre parole, il sistema operativo cerca il blocco libero di memoria più grande tra tutti quelli disponibili e lo assegna al processo.

Il processo di allocazione della memoria Worst-Fit funziona nel seguente modo:
\begin{itemize}
    \item Il sistema operativo cerca la lista di tutti i blocchi liberi di memoria disponibili.
    \item Il sistema operativo cerca il blocco di memoria libero più grande che possa soddisfare la richiesta del processo.
    \item Se viene trovato un blocco libero con dimensioni esattamente corrispondenti alla richiesta del processo, il sistema operativo lo alloca al processo.
    \item Se viene trovato un blocco libero con dimensioni maggiori della richiesta del processo, il sistema operativo assegna il blocco di memoria al processo e crea un nuovo blocco libero contenente la memoria non utilizzata.
    \item Se non viene trovato alcun blocco libero di memoria sufficiente per soddisfare la richiesta del processo, il processo viene messo in attesa fino a quando non viene rilasciata una porzione di memoria sufficiente.
\end{itemize}
Il vantaggio dell'algoritmo Worst-Fit è che consente di utilizzare lo spazio di memoria disponibile in modo efficiente, poiché utilizza il blocco libero di memoria più grande possibile. Tuttavia, questo algoritmo può portare ad una \textbf{frammentazione} della memoria, poiché può lasciare molti piccoli blocchi liberi di memoria dopo l'allocazione di grandi blocchi, rendendo più difficile l'allocazione di blocchi liberi di dimensioni maggiori in futuro.

\subsection{Frammentazione interna ed esterna}


La \textbf{frammentazione} della memoria si verifica quando la memoria disponibile non è contigua e non può essere utilizzata per soddisfare le richieste di memoria dei processi.

Esistono due tipi di frammentazione della memoria: la frammentazione \textbf{interna} e la frammentazione \textbf{esterna}:
\begin{itemize}
    \item La \textbf{frammentazione interna} si verifica quando viene sprecata parte di memoria a seguito dell'allocazione di settori troppo grandi per un processo richiedente meno spazio di quello fornito. Si verifica nel caso in cui si utilizzi un'\textbf{allocazione statica} della memoria tramite la definizione di partizioni.
    
    \item La \textbf{frammentazione esterna} dove vi è abbastanza spazio libero per poter allocare il processo, ma tale spazio è suddiviso in buchi di memoria non contigui, impedendo l'allocazione. Si verifica nel caso in cui si utilizzi un'\textbf{allocazione dinamica} della memoria a seguito del frequente allocamento e deallocamento di processi.
\end{itemize}

Se si utilizza un'allocazione statica dei settori, la frammentazione interna non può essere risolta completamente, ma si può ridurre creando \textbf{settori} di dimensioni \textbf{crescenti} per permettere l'allocazione dei processi meno costosi nei settori più piccoli.\par
\pagebreak
Se invece si utilizza un'allocazione dinamica, il problema della frammentazione esterna può essere risolto in due modi:
\begin{itemize}
    \item \textbf{Compattazione totale}: tutti i processi vengono spostati in modo che siano contigui gli uni agli altri, creando un unico buco di memoria.
    \item \textbf{Compattazione parziale}: solo alcuni processi vengono spostati per creare un buco sufficientemente grande da poter contenere il processo da allocare.
\end{itemize}


\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/compa1}}
        \caption{Compattazione totale}
    \end{center}
\end{figure}



\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/compa2}}
        \caption{Compattazione parziale}
    \end{center}
\end{figure}

\subsection{Swapping}
Finora, abbiamo ipotizzato che tutti i processi siano interamente caricati in memoria.\footnote{Ricorda: un processo deve risiedere fisicamente nella memoria principale solo se la CPU esegue le sue istruzioni e accede ai suoi dati}. Se un processo si blocca (ad esempio, a causa di una chiamata I/O) non è necessario che sia in memoria mentre l'I/O è in esecuzione. Quel processo può essere "swapped out" dalla memoria al disco per fare spazio ad altri processi. \par
Una volta che il processo è di nuovo pronto, il sistema operativo deve ricaricarlo in memoria. Lo scambio dipende dall'associazione di indirizzi utilizzata:
\begin{itemize}
    \item \textbf{tempo di compilazione o caricamento}: devono essere reinseriti nella stessa posizione di memoria da cui sono stati restituiti
    \item \textbf{tempo di esecuzione}: può essere ricollocato in qualsiasi posizione disponibile (aggiornamento dei registri di base e di limite)
\end{itemize}
Utilizzando lo scambio, la frammentazione può essere affrontata facilmente. Basta eseguire la compattazione prima di eseguire lo scambio in un processo.
\pagebreak
\subsubsection*{Esempio}
Lo scambio è un processo molto lento rispetto ad altre operazioni a causa dell'interazione con il disco rigido. Abbiamo un processo utente da 10 MB, velocità di trasferimento del disco = 40 MB/sec (250 msec solo per il trasferimento dei dati). Poiché lo swap-in può comportare lo swap-out di un altro processo, il tempo complessivo richiesto sarà di $\approx 500$ msec. 

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.4\textwidth,keepaspectratio]{{im/swap}}
        \caption{La maggior parte dei sistemi operativi moderni non utilizza più lo scambio, perché è troppo lento e sono disponibili alternative più veloci (ad esempio, paging)}
    \end{center}
\end{figure}

\subsection{Paginazione}

La \textbf{paginazione} è una tecnica di gestione della memoria utilizzata dai sistemi operativi per permettere ai processi di utilizzare più memoria di quella fisicamente disponibile nel sistema. In pratica, quando un processo cerca di accedere a una determinata area di memoria, il sistema operativo verifica se la pagina di memoria a cui tale area appartiene è già presente nella memoria fisica del computer.\par

Se la pagina è già presente, il processo può accedere alla memoria immediatamente. Se la pagina non è presente, il sistema operativo deve recuperarla dalla memoria virtuale e copiarla nella memoria fisica, liberando eventualmente una pagina già presente se non ci sono pagine disponibili. Questo processo viene chiamato "\textbf{fault di pagina}".\par

L'allocazione contigua non è più necessaria poiché le pagine logiche possono essere mappate a pagine con frame fisici non contigui. Inoltre la \textbf{frammentazione esterna} viene \textbf{eliminata} perché le pagine hanno dimensioni fisse, mentre la \textbf{frammentazione interna} può comunque \textbf{verificarsi}.\par

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.4\textwidth,keepaspectratio]{{im/paginazione}}
        \caption{}
    \end{center}
\end{figure}

Responsabilità di base del sistema operativo per il paging:
\begin{itemize}
    \item \textbf{mapping} tra pagine logiche e frame fisici
    \item \textbf{traduzione} di indirizzi logici in indirizzi fisici
\end{itemize}
Il sistema operativo necessita di una Page Table per fare ciò. La dimensione della page table (ovvero il numero di entry) dipende dalla dimensione fissata delle pagine.\par

Quando un processo cerca di accedere a un indirizzo di memoria, il sistema operativo utilizza la tabella delle pagine (\textbf{page table}) per \textbf{tradurre} l'indirizzo virtuale del processo in un indirizzo fisico corrispondente. Ogni indirizzo virtuale è composto da due parti: un numero di pagina $p$ che indica in quale pagina virtuale si trova l'indirizzo e un \textbf{offset} che indica la posizione dell'indirizzo all'interno della pagina.\par

Analogamente, ogni indirizzo fisico è composto da un numero di frame $f$ che indica in quale frame fisico si trova la pagina corrispondente all'indirizzo virtuale e un offset che indica la posizione dell'indirizzo all'interno del frame.

Durante la conversione da indirizzo virtuale a fisico, il sistema operativo utilizza la pagina indicata dal numero di pagina $p$ dell'indirizzo virtuale per trovare il corrispondente frame fisico $f$ nella tabella delle pagine. In questo modo, l'offset dell'indirizzo virtuale rimane lo stesso e viene utilizzato anche nell'indirizzo fisico corrispondente. In sintesi, la traduzione degli indirizzi virtuali in indirizzi fisici avviene utilizzando la page table, in cui ogni pagina virtuale è associata a un frame fisico corrispondente.


\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.45\textwidth,keepaspectratio]{{im/indvir}}
        \caption{}
    \end{center}
\end{figure}

La paginazione, dunque, corrisponde semplicemente ad una forma di \textbf{rilocazione dinamica} dove ogni indirizzo virtuale è legato ad un indirizzo fisico tramite la page table, la quale può essere interpretata come un insieme di registri base (uno per ogni frame).


\subsubsection*{Esempi}

\begin{enumerate}
    \item Prendiamo il seguente esempio:
    \begin{itemize}
        \item Supponiamo che lo spazio di memoria riservato ai processi utente sia $M$ = $50B$ e che la dimensione di ogni pagina/frame sia $S$ = $10B$
        \item Consideriamo l'indirizzo virtuale $x$ = 27. Il page number sarà $p$ = $\lfloor \frac{x}{S}\rfloor$  = $\lfloor \frac{27}{10}\rfloor$ = 2, 
        mentre l'offset di word sarà $o$ = 27 mod 10 = 7.
        \item Dunque, l'indirizzo fisico sarà costituito dal numero di frame contenuto nella seconda pagina della page table e un offset pari a 7
    \end{itemize}
    \item Prendiamo il secondo esempio:
    \begin{itemize}
        \item Supponiamo di avere una memoria virtuale e una memoria fisica entrambe di dimensione $M$ = 1024$B$ dove il word size è $W$ = 2$B$. Supponiamo inoltre di utilizzare un paginazione con pagine/frame di dimensione $S$ = 16$B$.
        \item  Il numero di pagine/frame contenute page table sarà T = $\frac{M}{S}$ = $\frac{1024}{16}$ = 64 dunque saranno necessari $p$ = 6 bit per poter indicizzare ogni pagina della page table ($2^{6}$ = 64)
        \item Il numero di word contenute in ogni frame sarà F = $\frac{S}{W}$ = $\frac{16}{2}$ = 8 , dunque saranno necessari $o$ = 3 bit per poter indicizzare il word offset ($2^{3}$ = 8).
        \item Dei 10 bit costituenti un indirizzo fisico, quindi, 6 saranno utilizzati per il
        numero di pagina, 3 per l'offset di word e il restante bit per l'offset di byte.
    \end{itemize}
\end{enumerate}
\pagebreak
\subsection{Translation Look-aside Buffer (TLB)}
La \textbf{Translation Look-aside Buffer (TLB)} è una cache di primo livello che viene utilizzata per memorizzare i numeri di pagina e di frame calcolati precedentemente. La TLB contiene i numeri più frequentemente utilizzati o più vicini a quelli già calcolati. Tuttavia, poiché le cache sono più piccole della memoria principale, la TLB conterrà solo una parte dei valori contenuti nella page table.\par

Quando viene richiesto un indirizzo virtuale, il sistema controllerà prima se il numero di pagina associato a tale indirizzo è già presente nella TLB. In caso affermativo (\textbf{TLB hit}), il frame associato verrà subito restituito. Al contrario, se il numero di pagina non è presente nella TLB (\textbf{TLB miss}), il controllo verrà effettuato sulla \textbf{page table} e il dato letto verrà caricato nella TLB per utilizzi futuri.\par

La TLB è \textbf{condivisa} tra tutti i processi, il che significa che lo stesso numero di pagina può essere mappato su un differente numero di frame in base al processo richiedente.

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.8\textwidth,keepaspectratio]{{im/tlb}}
        \caption{}
    \end{center}
\end{figure}
\pagebreak
\subsection{Segmentazione}
La \textbf{segmentazione} della memoria è una tecnica utilizzata nello sviluppo di applicazioni per semplificare l'organizzazione della memoria in modo \textbf{non contiguo}. Con questa tecnica, la memoria di un programma viene suddivisa in segmenti, ciascuno dei quali è dedicato ad uno specifico tipo di dati o codice. Gli indirizzi utilizzati in questa tecnica sono costituiti da un \textbf{numero di segmento} e un \textbf{offset} dall'inizio del segmento, permettendo ai programmatori di ragionare in termini di segmenti anziché di singoli byte.\par

In pratica, quando un programma richiede l'accesso ad un dato o ad una istruzione, viene utilizzato l'indirizzo del segmento corrispondente al tipo di dato o di codice richiesto, insieme all'offset dall'inizio del segmento. In questo modo, il programma può accedere ai propri dati e istruzioni in modo più intuitivo e organizzato, senza dover gestire manualmente l'allocazione della memoria in modo contiguo.\par

In pratica, la \textbf{segment table} è una struttura dati utilizzata dal sistema operativo per gestire la segmentazione della memoria. Essa \textbf{associa} ogni indirizzo costituito da un numero di segmento ed un offset ad un indirizzo fisico, verificando inoltre se l'indirizzo sia valido o meno.
Ogni voce della segment table contiene informazioni sull'\textbf{indirizzo base} del segmento, la \textbf{lunghezza} del segmento e \textbf{informazioni aggiuntive} per la protezione del segmento. A \textbf{differenza} della page table, la segment table può essere implementata utilizzando pochi registri base e limite. In questo modo, viene semplificata la gestione della memoria segmentata e si riduce il costo computazionale associato alla traduzione degli indirizzi.

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.5\textwidth,keepaspectratio]{{im/stable}}
        \caption{}
    \end{center}
\end{figure}

La segmentazione è comoda per la gestione dei dati condivisi, ma \textbf{poco efficiente} nell'uso della \textbf{memoria}. Per questo motivo, la tecnica ottimale consiste nell'applicare la paginazione ai segmenti stessi, in modo da avere una gestione più efficiente della memoria. In pratica, lo spazio di indirizzamento virtuale di ogni processo viene diviso in segmenti, e lo spazio di indirizzamento fisico viene diviso in frame di dimensioni fisse. 
\begin{itemize}
    \item Ogni processo ha la propria tabella dei segmenti 
    \item Ogni segmento ha la propria tabella delle pagine 
\end{itemize}
\pagebreak
Poiché i segmenti sono generalmente più grandi dei frame fisici. In questo modo, ogni segmento logico può essere mappato in più frame fisici, garantendo un'efficiente gestione della memoria.
    

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/seg1}}
        \caption{}
    \end{center}
\end{figure}


Affinché la traduzione da indirizzo virtuale a fisico sia possibile, quindi, ogni indirizzo deve essere \textbf{composto} da un \textbf{numero di segmento}, \textbf{un numero di pagina} legato alla page table del segmento specifico ed un \textbf{offset} dall'inizio del frame indicato nella page table.\par

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/seg2}}
        \caption{}
    \end{center}
\end{figure}


Per tenere traccia delle segment table e delle page table dei vari processi, vengono utilizzate due soluzioni:
\begin{itemize}
    \item Vengono utilizzati pochi registri per le segment table, mentre le page table vengono salvate nella memoria principale con aggiunta di una TLB. Tale soluzione risulta più veloce, ponendo tuttavia un limite al numero di segmenti
    \item Sia le segment table sia le page table vengono salvate nella memoria principale con aggiunta di una TLB, i cui controlli vengono effettuati utilizzando un indice dei segmenti ed un indice di pagine. Tale soluzione risulta essere più lenta ma più flessibile.
\end{itemize}
    
\pagebreak
Presenta dei vantaggi e dei svantaggi:
\begin{itemize}
    \item Tra i vantaggi:
    \begin{itemize}
        \item Il compilatore e l'OS vedono la memoria in modo analogo
        \item Flessibilità
        \item Assenza di frammentazione esterna
        \item Condivisione della memoria tra i processi
    \end{itemize}
    \item Tra gli svantaggi:
    \begin{itemize}
        \item Context switch e traduzione degli indirizzi più lenti 
        \item Frammentazione interna ancora esistente
    \end{itemize}
\end{itemize}
Oltre alla paginazione segmentata, i moderni sistemi operativi sono dotati di una forme più avanzate di paginazione:
\begin{itemize}
    \item \textbf{Paginazione a due livelli}, dove una page table viene suddivisa in più page table (dunque si tratta di una paginazione della page table)
    \item \textbf{Paginazione con hash}, dove vengono utilizzate delle hash table per indicizzare più page table sparse in modo veloce
    \item \textbf{Page table invertita}, dove vengono conservati i numeri di tutti frame attualmente carichi in memoria
\end{itemize}


\subsubsection{Paginazione vs Segmentazione}
\begin{center}
    
    \begin{tabular}{lll}
        \toprule
        \textbf{Considerazione} & \textbf{Paginazione} & \textbf{Segmentazione}\\
        \midrule
        Il programmatore deve sapere che\\ questa tecnica è in uso? & No & Si\\
        \addlinespace
        \midrule
        Quanti spazi di indirizzi \\lineari ci sono? & 1 & Molti\\
        \addlinespace
        \midrule
        
        Lo spazio degli indirizzi\\ totale può superare la \\dimensione della memoria fisica? & Si & Si \\
        \addlinespace
        \midrule
        
        Le procedure e i dati\\ possono essere distinti e \\protetti separatamente? & No & Si \\
        \addlinespace
        \midrule
        
        Le tabelle la cui \\dimensione varia possono \\essere disposte facilmente? & No & Si \\
        \addlinespace
        \midrule
        
        La condivisione delle \\procedure fra utenti \\è facilitata? & No & Si\\
        \addlinespace
        \midrule
        
        Perché fu inventata questa tecnica? &  
        \makecell[l]{Per avere uno spazio\\
        degli indirizzi\\
        lineare grande\\
        senza dover\\
        acquistare\\
        ulteriore\\
        memoria fisica} & 
        \makecell[l]{Per consentire a \\programmi e dati di essere\\
        spezzati in spazi\\ degli indirizzi
        logicamente \\indipendenti
        e per \\facilitare la condivisione\\ e la protezione}\\
        \bottomrule
    \end{tabular}
\end{center}
    
\pagebreak
\subsection{Memoria Virtuale}

La maggior parte dei processi reali non ha bisogno che tutte le pagine vengano caricate in memoria, o almeno non tutte in una volta, come il codice di gestione degli errori, gli array sono spesso sovradimensionati per gli scenari peggiori (e nella pratica viene effettivamente utilizzata solo una piccola parte degli array) oppure alcune funzionalità di determinati programmi vengono utilizzate raramente. La memoria virtuale consente di mantenere allocate in memoria fisica solo alcune pagine dello spazio di indirizzamento logico di un processo.
La \textbf{memoria virtuale} utilizza l'archiviazione di supporto (ovvero il disco) per archiviare le pagine inutilizzate e dare l'illusione di uno spazio di indirizzi virtuale infinito.\par
La possibilità di caricare dal disco solo le porzioni di processi effettivamente \textbf{necessarie} (e solo quando necessarie) presenta numerosi \textbf{vantaggi} infatti i programmi potrebbero essere scritti per uno spazio di indirizzi molto più ampio di quello che esiste fisicamente sul computer e viene lasciata più memoria per altri programmi, migliorando l'utilizzo della CPU. È necessario, perciò, meno I/O per scambiare i processi dentro e fuori la memoria, velocizzando le operazioni. \par
L'idea della memoria virtuale è di utilizzare la memoria come \textbf{cache} per il disco. La tabella delle pagine deve anche indicare se la pagina è su disco o in memoria (usando solo un singolo bit non valido). Una volta che la pagina è stata caricata dal disco alla memoria, il sistema operativo aggiorna la voce corrispondente della tabella delle pagine insieme al bit valido. \footnote{Ricorda: l'accesso al disco è estremamente più lento dell'accesso alla memoria, pertanto, gli accessi alla memoria devono fare riferimento a pagine che sono in memoria con alta probabilità}
Tramite la regola del \textbf{90/10}, ossia la regola affermante che un processo spende il 90\% del tempo \textbf{accedendo} solo al 10\% della sua memoria \textbf{allocata}, sappiamo che la maggior parte dei riferimenti alla memoria effettuati da un processo si trova in una piccola zona di memoria, che definiamo come \textbf{working set} del processo.\par
Siccome tale zona è molto piccola, la probabilità che essa possa essere caricata in memoria è molto alta. Ovviamente, durante l'esecuzione di un processo, il working set potrebbe variare. Tuttavia, tale variazioni avvengono con frequenza bassa, dunque per un determinato lasso di tempo il working set rimarrà lo stesso.

\subsubsection{Gestione page fault}
Ad ogni riferimento di memoria logica, viene eseguita una ricerca nella tabella delle pagine. Nella tabella delle pagine, il bit \textit{valid-invalid} viene controllato per la voce corrispondente. Se il bit è impostato a 1 significa che l'inserimento della pagina è valido (ovvero, la pagina richiesta è in memoria). In caso contrario, si verifica una trap di errori di pagina e la pagina deve essere caricata (ovvero recuperata) dal disco.

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/pagf}}
        \caption{Gestione dei Page Fault}
    \end{center}
\end{figure}


\begin{enumerate}
    \item L'indirizzo di memoria viene prima controllato, per vedere se è legittimo
    \item Se l'indirizzo è legittimo, la pagina deve essere recuperata dal disco
    \item Viene individuato un frame libero, possibilmente da un elenco di frame liberi (il sistema operativo potrebbe dover scegliere un frame da scaricare se tutta la memoria è piena)
    \item È pianificata un'operazione del disco per portare la pagina necessaria dal disco (questo bloccherà il processo in attesa di I/O, consentendo l'esecuzione di qualche altro processo)
    \item Al termine dell'operazione di I/O, la tabella delle pagine del processo viene aggiornata con il nuovo numero di frame e il bit viene impostato su valido
    \item Il processo corrente viene interrotto e l'istruzione che ha causato l'errore di pagina deve essere riavviata dall'inizio
\end{enumerate}

La Gestione degli errori di pagina utilizza:

\begin{itemize}
    \item \textbf{Hit TLB}, ovvero il TLB utilizza il bit valido per indicare il fatto che la pagina è nella memoria principale. Se otteniamo un TLB hit ma il frame non è effettivamente nella memoria principale, dobbiamo comunque andare a recuperare la pagina dal disco. Il TLB hit significa che la voce della pagina richiesta è nella cache e anche il frame di riferimento è in memoria.
    \item Se la pagina richiesta non è nella cache abbiamo un \textbf{TLB miss} ma è in memoria, il sistema operativo seleziona una voce TLB da sostituire e la riempie con la nuova voce. Se la pagina richiesta non è nella cache (TLB miss) e non è nemmeno in memoria (ovvero, si trova su disco) il sistema operativo seleziona una voce TLB da sostituire e la riempie con la nuova voce invalidando la voce TLB come segue:
    \begin{itemize}
        \item esegue operazioni di page fault trap
        \item aggiorna la voce TLB
        \item riavvia l'istruzione in errore
    \end{itemize}
\end{itemize}

\subsubsection{Istruzione idempotente}
Un'\textbf{istruzione idempotente} è quella che può essere eseguita più volte \textbf{senza causare} alcun effetto diverso dalla sua prima esecuzione. Se questa istruzione viene interrotta a causa di un errore come un page fault, può essere facilmente ripetuta senza causare alcun danno.\par
Al contrario, si dice un'\textbf{istruzione non idempotente}, se la sua riesecuzione dopo un errore può essere molto più difficile poiché potrebbe modificare lo stato del sistema in modo imprevedibile. In questo caso, l'errore potrebbe essere difficile da gestire e potrebbe richiedere un approccio più complesso per correggere il problema.
Il \textbf{principio di località} dei riferimenti è un principio fondamentale dell'organizzazione della memoria dei computer. Esso afferma che, quando un processo accede a un determinato indirizzo di memoria, è probabile che acceda di nuovo allo stesso indirizzo in futuro (principio temporale) e che acceda anche a indirizzi vicini a quello iniziale (\textbf{principio spaziale}).

Questo significa che, anche se teoricamente un page fault potrebbe verificarsi a seguito di ogni istruzione eseguita, nella pratica ci sono buone probabilità che il sistema operativo debba gestire i page fault solo quando il processo in esecuzione accede a una nuova pagina di memoria. Ciò accade perché il processo ha già accesso a molte delle pagine di memoria richieste, o ad altre pagine che si trovano vicino ad esse, grazie al principio di località dei riferimenti.

In questo modo, il principio di località dei riferimenti aiuta a minimizzare il numero di page fault, riducendo l'overhead del sistema operativo e migliorando le prestazioni complessive del sistema.
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.8\textwidth,keepaspectratio]{{im/pformula}}
        \caption{Gestione dei Page Fault}
    \end{center}
\end{figure}
\pagebreak
\subsubsection{Esempio}
\begin{itemize}
    \item Supponiamo che $t_{MA}$ = 100 ns, $t_{FAULT}$ = 20ms = 20000000ns e che la probabilità di un page fault sia p = 0.0001 = $\frac{1}{10000}$. In tal caso si ha che:\par
    \begin{equation}
        t_{ACCESS} = (1 - \frac{1}{10000})\cdot 100ns + \frac{1}{10000} \cdot 20000000ns \frac{9999}{100}ns+ 2000ns \approx 20.1 \mu sec
    \end{equation}
    \item  Nel caso in cui volessimo ottenere un tempo di accesso al massimo il 10\% più lento di un normale accesso in memoria, ci basterebbe risolvere la seguente equazione:
    \begin{equation}
        1.1\cdot100=(1-p)\cdot100+p\cdot20000000 \iff p= 1 \approx 5\cdot10-7 1999990
    \end{equation}
    Di conseguenza, possiamo tollerare un massimo di un page fault ogni circa 2 milioni di accessi
\end{itemize}

Più generalmente, dati $t_{MA}$, $t_{FAULT}$ e una soglia $\epsilon >$ 0 tale che:
\begin{equation}
    t_{ACCESS} =(1+\epsilon) \cdot t_{MA}
\end{equation}

Per trovare il numero di page fault tollerabili sarà $\frac{1}{p}$, dove si ha che:\par
\begin{center}
    $t_{ACCESS} = (1+\epsilon) \cdot t_{MA} \iff (1-p) \cdot t_{MA} + p \cdot t_{FAULT}$ =\par
    $(1+\epsilon) \cdot t_{MA}\iff p(t_{FAULT} - t_{MA}) = \epsilon \cdot t_{MA} \iff p = \frac{\epsilon \cdot t_{MA}}{t_{FAULT} - t_{MA}}$
\end{center}
    
\pagebreak
\subsection{Recupero e rimpiazzo delle pagine}

Principalmente, esistono tre \textbf{strategie} per poter \textbf{recuperare} le pagine dal disco:
\begin{itemize}
    \item \textbf{Recupero all'avvio}, dove tutte le pagine del processo vengono caricare contemporaneamente
    \item \textbf{Recupero ad overlay}, dove è il programmatore a scegliere quando e quali pagine da caricare
    \item \textbf{Recupero a richiesta}, dove un processo comunica all'OS quando necessita che una pagina venga recuperata.
\end{itemize}
Il recupero a richiesta è il sistema utilizzato da tutti gli OS moderni. Quando un processo viene avviato, nessuna delle sue pagine è carica in memoria, venendo spostate solamente quando il processo richiede di accedervi (dunque tramite i page fault). Tale sistema di recupero delle pagine viene detto \textbf{lazy swapper} o pager.\par
Un'ulteriore tecnica utilizzata per il recupero delle pagine è il \textbf{prefetching}, dove il pager predice quando le pagine verranno richieste, caricandole in anticipo. Tale tecnica, tuttavia, risulta prettamente impraticabile, poiché predire le future pagine risulta molto difficile. Un possibile approccio consiste nel caricare più di una singola pagina durante un page fault (efficace solo se il programma accede sequenzialmente alla memoria).\par

Nel caso in cui debba essere caricata una pagina dal disco ma la memoria fisica sia piena, è necessario \textbf{rimpiazzare} una delle pagine caricate. Gli algoritmi più utilizzati per tale compito sono i seguenti:
\begin{itemize}
    \item Rimpiazzo casuale, dove viene rimossa una pagina a caso
    \item \textbf{FIFO} (First in, First out), dove viene rimossa la pagina rimasta in memoria per
    il tempo maggiore. Nell'algoritmo \textbf{second chance} la pagina che dovrebbe essere eliminata viene rimessa in fondo all'elenco e il suo tempo di caricamento aggiornato come se fosse appena arrivata in memoria. Questo secondo algoritmo risulta inefficiente poichè fa scorrere di continuo le pagine lungo la sua lista.
    \item L'algoritmo detto \textbf{clock} tiene i frame su una lista circolare a forma di orologio. La lancetta indica la pagina più vecchia. Quando avviene un page fault, la pagina indicata dalla lancetta viene controllata. Se il suo bit \textit{R} è 0 allora viene sfrattata, la nuova pagina inserita al suo posto nell'orologio, e la lancetta spostata in avanti di una posizione. Il reference bit viene impostato a 0 e l'algoritmo prosegue ad esaminare il frame successivo nella lista. Se il suo bit \textit{R} è \ viene azzerrato e la lancetta avanzata alla pagina successiva. 
    \item \textbf{MIN (OPT)}, dove viene rimossa la pagina che non verrà acceduta per il lasso di tempo maggiore (richiede di predire il futuro, dunque è un algoritmo ottimale ma molto difficile da implementare)
    \item \textbf{LRU (Least Recently Used)\footnote{Indipendentemente da come LRU venga implementato, noi facciamo riferimento all'algoritmo LRU esatto che è piuttosto semplice. Dato lo stato attuale della memoria (piena) e una nuova richiesta di una pagina che non è presente in memoria, si rimpiazza quella pagina che è stata riferita meno recentemente. 
    Ad esempio, supponiamo che la memoria, a un certo istante t, contenga i frame: A, B, C. Inoltre, la memoria è piena, per cui se all'istante t+1 viene richiesta la pagina D (che non si trova in memoria!) le si deve "far posto", rimpiazzandola con uno dei frame allocati, ossia uno tra A, B e C.
    A questo punto, LRU controlla cos'è accaduto in passato: t-1, t-2, ...
    Supponiamo che la sequenza di accessi fino al tempo t-1 sia: B, C, C, A, B, C, A
    Dobbiamo trovare il frame che è stato riferito meno recentemente. Ebbene, A è stato riferito al tempo t-1, C al tempo t-2 e B al tempo t-3. Ne segue che la pagina "sacrificabile" è proprio B, in quanto è stata richiesta più indietro nel tempo. Fonte: Prof}}, dove viene rimossa la pagina non utilizzata per il maggior lasso di tempo. È l'algoritmo più utilizzato dagli OS moderni e può essere implementato in tre modi:
    \begin{itemize}
        \item Tramite un \textbf{singolo bit di riferimento}, dove viene mantenuto un singolo bit per ogni entrata della page table, il quale viene impostato a 0 inizialmente (dunque anche dopo ogni pulizia della page table) e viene impostato ad 1 nel caso in cui venga fatto un accesso alla pagina associata.
        Durante la rimozione, viene scartata una delle pagine impostate a 0.
        \item Tramite \textbf{più bit di riferimento}, dove vengono utilizzati 8 bit per ogni entrata della page table, i quali vengono shiftati di un bit ad ogni colpo di clock. Dopo ogni accesso alla pagina associata viene impostato ad 1 il bit più a sinistra.
        Durante la rimozione, viene scartata la pagina avente il più basso valore negli 8 bit.
        \item Tramite \textbf{l'algoritmo di seconda chance}, dove viene utilizzato un singolo bit di riferimento ed una politica FIFO: l'OS si occupa di tenere traccia dei frame in una lista FIFO circolare e dove ad ogni accesso alla memoria viene impostato il bit di riferimento ad 1.\par

        Durante un page fault, l'OS analizza l'intera lista delle page table, controllando i bit di riferimento del frame: se il bit è impostato su 0, allora la pagina viene rimpiazzata ed esso viene impostato su 1, altrimenti, esso viene impostato su 0 (seconda chanche) e viene ripetuto il controllo sul frame successivo.
        Una versione più avanzata di tale algoritmo prevede l'uso di un ulteriore bit di modifica (se 1 allora la pagina è diversa da quella salvata sul disco, altrimenti no), classificando le pagine in quattro categorie:
        \begin{enumerate}
            \item Se i bit sono (1,1), allora la pagina è stata utilizzata recentemente ed è stata modificata
            \item Se i bit sono (1,0), allora la pagina è stata utilizzata recentemente ma non è stata modificata
            \item Se i bit sono (0,1), allora la pagina non è stata utilizzata recentemente ma è stata modificata
            \item Se i bit sono (0,0), allora la pagina non è stata utilizzata recentemente e non è stata modificata
        \end{enumerate}
        Durante la rimozione, viene scartata la prima pagina trovata avente categoria inferiore (dunque prima (0,0), poi (0,1), \dots)
    \end{itemize}
\end{itemize}
\pagebreak

\subsection{Trashing e determinazione del working set}

Definiamo come \textbf{trashing} il fenomeno in cui la memoria è utilizzata da troppi processi contemporaneamente, portando le pagine ad essere continuamente rimpiazzate nonostante esse siano ancora in uso dai processi stessi e dunque una forte degradazione delle performance. 

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/trashing}}
        \caption{}
    \end{center}
\end{figure}


Per contrastare il trashing, è necessario cedere abbastanza memoria ad ogni processo, rendendo quindi fondamentali le politiche di allocazione e rimpiazzo delle pagine:
\begin{itemize}
    \item \textbf{Allocazione e rimpiazzo globale}, dove tutte le pagine di ogni processo sono in una singola pool, ossia una queue LRU dove, durante il rimpiazzo di una pagina, anche le pagine non appartenenti allo stesso processo della nuova pagina possono essere rimpiazzate, portando ad una maggiore flessibilità ma anche ad una maggiore quantità di trashing
    \item \textbf{Allocazione e rimpiazzo locale}, dove ogni processo possiede la propria pool, dunque il rimpiazzo LRU controllerà solo le pagine appartenenti allo stesso processo della nuova pagina, portando ad un totale isolamento dei processi ma anche ad una minore performance
    \item \textbf{Allocazione e rimpiazzo proporzionale}, dove viene assunto che ogni processo di grandi dimensioni faccia riferimento anche ad una grande quantità di memoria. Tuttavia, in alcuni casi tale assunzione si rivela errata (ad esempio, un processo potrebbe allocare un array di 1 GB, per poi utilizzarne solo una piccola porzione). In altre parole, non sempre il working set di un processo è correlato al suo impatto sulla memoria.
\end{itemize}
L'obiettivo, quindi, è quello di riuscire a dare ad ogni processo abbastanza frame da poter contenere il proprio working set.

Informalmente, il \textbf{working set} di un processo è l'insieme delle pagine che il processo sta attualmente utilizzando. Formalmente, esso corrisponde all'insieme di tutte le pagine a cui esso ha effettuato riferimenti durante le ultime T unità di tempo.
La scelta della grandezza della finestra di riferimenti di cui tenere traccia, indicata con $\delta$, risulta fondamentale: una finestra troppo piccola non racchiude tutte le pagine della località attualmente richiesta dal processo, mentre una finestra troppo grande include pagine che non sono più accedute frequentemente.\par
Per evitare di dover gestire una lista delle ultime $\delta$ pagine richieste, il working set viene spesso implementato tramite una semplice campionatura: ogni k riferimenti in memoria, il working set di un processo viene considerato l'insieme di tutte le pagine a cui è stato fatto riferimento in tale periodo temporale.

\pagebreak
\section{Gestione dell'archiviazione}


Principalmente, i dispositivi di archiviazione di massa si suddividono in tre categorie: 
\begin{itemize}
    \item dischi magnetici 
    \item dischi a stato solido
    \item nastri magnetici. 
\end{itemize}
Le prime due categorie vengono dette memorie secondarie, mentre la rimanente viene detta memoria terziaria.
Un disco magnetico è una memoria secondaria composta da uno o più piatti ricoperti di materiale magnetico (metallo rigido per gli HDD, plastica flessibile per i floppy disk), dove:
\begin{itemize}
    \item Ogni piatto possiede due superfici funzionali, ossia il fronte e il retro, entrambe suddivise in un numero di anelli concentrici, detti tracce. L'insieme di tutte le tracce di ogni piatto poste alla stessa distanza dal bordo viene detto cilindro.
    \item Ogni traccia è a sua volta suddivisa in settori (solitamente da 512 byte ciascuno), ognuno di essi contenente un'intestazione, un trailer e informazioni di verifica. Una dimensione maggiore per i settori riduce la quantità di spazio sprecata per contenere le intestazioni e i trailer, ma aumenta la frammentazione interna
    \item Ogni superficie viene letta da una testina di lettura e scrittura agganciata alla fine di un braccio, a sua volta connesso ad un attuatore, il quale muove tutti i bracci simultaneamente da una traccia all'altra
\end{itemize}
La capacità totale di memoria di un disco rigido corrisponde a: 
\begin{equation}
    C = H \cdot T \cdot S \cdot B
\end{equation}
dove $H$ è il numero di superfici, $T$ il numero di tracce per superficie, $S$ il numero di settori per traccia e $B$ il numero di byte per settore\par


\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/disk}}
        \caption{}
    \end{center}
\end{figure}

La CPU è in grado di interfacciarsi con i dischi magnetici tramite un bus I/O, un canale di comunicazione dove la CPU stessa impartisce comandi al disco tramite un controller posto posto alla fine del bus, ossa l'host controller, il quale a sua volta comunica con il disk controller interno al disco stesso, trasferendo i dati dalle superfici magnetiche del disco prima ad una cache interna al disco, per poi venir trasferiti alla memoria.
Per via della struttura di un disco magnetico, il tempo di trasferimento dei dati alla memoria principale, indicato come DTT, equivale alla somma tra:
\begin{itemize}
    \item \textbf{Tempo di posizionamento (Seek time)}, indicato come ST, ossia il tempo necessario per spostare le testine su una specifica traccia. Indica il tempo necessario al disco per posizionare le proprie testine su uno specifico \textbf{cilindro}.
    \item \textbf{Delay di rotazione}, indicato come ROT, ossia il tempo necessario affinché il settore desiderato venga ruotato sotto la testina (in media viene effettuata mezza rotazione del disco)
    \item \textbf{Tempo di trasferimento}, indicato come TT, ossia il tempo necessario per spostare i dati dal disco alla memoria, spesso espresso come rateo di trasferimento (larghezza di banda), ossia come byte al secondo
\end{itemize}

\begin{equation}
    DTT = ST + ROT + TT
\end{equation}

\subsection{Esempi}
\begin{itemize}
    \item Supponendo un tempo di trasferimento totale pari a 40ms per effettuare una particolare operazione I/O, dove il seek time è 18ms, il rotational delay è 7ms e che il trasfer rate è 5 Gbit/s, la quantità totale di dati trasferita è:
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/disk2}}
            \caption{}
        \end{center}
    \end{figure}
    \item Supponendo un tempo di trasferimento totale pari a 36ms per effettuare una particolare operazione I/O, dove il seek time è 13ms, il trasfer rate è 1 Gbit/s e sono stati trasferiti 2MB, il rotational delay è:
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/disk3}}
            \caption{}
        \end{center}
    \end{figure}
\end{itemize}

\subsection{Scheduling del disco}
Poiché il sistema operativo riceve costanti richieste di lettura e scrittura da parte dei pro cessi, è necessario gestire in modo efficiente gli accessi al disco, in particolare cercando di ridurre al minimo il seek time. Dunque, al fine di ridurre il seek time, è necessario ridurre al minimo la \textbf{distanza percorsa} dalla testina di lettura e scrittura.

\subsection{First Come First Served (FCFS)}
L'algoritmo \textbf{First Come First Served (FCFS)} è un algoritmo di scheduling del disco in cui le richieste di accesso al disco vengono effettuate nel loro ordine di arrivo nella coda di richieste.\par
Pro:
\begin{itemize}
    \item Facile da implementare
    \item Utilizzato anche dagli SSD poiché in tali tipologie di memorie non è richiesto alcun movimento meccanico, dunque il seek time è nullo
\end{itemize}
Contro:
\begin{itemize}
    \item Poco performante nel caso in cui il sistema abbia poche richieste
\end{itemize}
\subsubsection{Esempio}
\begin{itemize}
    \item Supponiamo di effettuare le richieste 65, 40, 18, 78 su un disco in cui la testina è inizialmente posizionata sulla traccia 30 utilizzando l'algoritmo FCFS:
    \begin{itemize}
        \item 
        \item La testina viene mossa da 30 a 65, dunque vengono traversate 35 tracce 
        \item La testina viene mossa da 65 a 40, dunque vengono traversate 25 tracce 
        \item La testina viene mossa da 40 a 18, dunque vengono traversate 22 tracce 
        \item La testina viene mossa da 18 a 78, dunque vengono traversate 60 tracce
    \end{itemize}
    \item La distanza percorsa, ossia il numero totale di tracce traversate, corrisponde a:
    \begin{center}
        35 + 25 + 22 + 60 = 142
    \end{center}
\end{itemize}
\subsection{Shortest Seek Time First (SSTF)}
L'\textbf{algoritmo Shortest Seek Time First (SSTF)} è un algoritmo di scheduling del disco in cui viene eseguita la richiesta di accesso al disco richiedente la minore quantità di tracce da traversare
Pro:
\begin{itemize}
    \item Può essere implementato mantenendo una semplice lista ordinata delle richieste
    \item Complessità non troppo elevata
\end{itemize}
Contro:
\begin{itemize}
    \item Potrebbe causare starvation (ad esempio, nel caso in cui una richiesta necessiti di una traccia troppo distante dalla testina)
    \item L'ottimizzazione avviene solo a livello locale (ad esempio, l'esecuzione della ri chiesta più vicina alla testina potrebbe portare ad una catena di spostamenti peggiore rispetto al caso in cui si vada a selezionare un'altra richiesta)
\end{itemize}


\subsubsection{Esempio}
\begin{itemize}
    \item Supponiamo di effettuare le richieste 65, 40, 18, 78 su un disco in cui la testina è inizialmente posizionata sulla traccia 30 utilizzando l'algoritmo SSTF:
    \begin{itemize}
        \item La testina viene mossa da 30 a 40, dunque vengono traversate 10 tracce 
        \item La testina viene mossa da 40 a 18, dunque vengono traversate 22 tracce 
        \item La testina viene mossa da 18 a 65, dunque vengono traversate 47 tracce 
        \item La testina viene mossa da 65 a 78, dunque vengono traversate 13 tracce
    \end{itemize}
    \item La distanza percorsa, ossia il numero totale di tracce traversate, corrisponde a:
    \begin{center}
        10 + 22 + 47 + 13 = 92
    \end{center}
\end{itemize}
\pagebreak
\subsection{SCAN e LOOK}
L'algoritmo \textbf{SCAN} è un algoritmo di scheduling del disco in cui la testina viene mossa da un estremo all'altro del piatto (ad esempio dalla traccia 0 alla tracca 100 e viceversa), dove le richieste vengono eseguite durante il passaggio della testina.\par
L'algoritmo \textbf{LOOK} corrisponde ad una versione ottimizzata dello SCAN, dove durante ogni scansione la testina, piuttosto che proseguire fino all'estremo del piatto, viene fermata alla richiesta più vicina all'estremo.

\subsubsection{Esempio}
\begin{itemize}
    \item Supponiamo di effettuare le richieste 65, 40, 18, 78 su un disco avente tracce numerate da 0 a 100 in cui la testina è inizialmente posizionata sulla traccia 30 utilizzando l'algoritmo SCAN:
    \begin{itemize}
        \item La testina viene mossa da 30 a 18, dunque vengono traversate 12 tracce
        \item La testina viene mossa da 18 a 0 per poter raggiungere l'estremo, dunque
        vengono traversate 18 tracce
        \item La testina viene mossa da 0 a 40, poi da 40 a 65 e infine da 65 a 78,dunque vengono traversate 78 tracce
    \end{itemize}
    \item La distanza percorsa, ossia il numero totale di tracce traversate, corrisponde a
   \begin{center}
    12 + 18 + 78 = 108
   \end{center}
    \item Se si fosse utilizzato l'algoritmo SCAN ottimizzato, ossia l'algoritmo LOOK, la distanza percorsa corrisponderebbe a:
    \begin{center}
        12+78 = 90
    \end{center}
\end{itemize}


\subsection{C-SCAN e C-LOOK}

L'algoritmo \textbf{C-SCAN} (o Circular SCAN) è un algoritmo di scheduling del disco simile all'algoritmo SCAN in cui a testina viene mossa dall'estremo finale fino all'estremo iniziale (ad esempio dalla traccia 100 alla traccia 0), venendo subito dopo "resettata" fino all'estremo finale (venendo spostata nuovamente sulla traccia 100). Le richieste vengono effettuate solo durante lo spostamento dall'estremo finale fino a quello iniziale.
L'algoritmo \textbf{C-LOOK} (o Circular LOOK) corrisponde ad una versione ottimizzata del C-SCAN, dove durante ogni scansione la testina, piuttosto che proseguire fino all'estremo iniziale del piatto, viene fermata alla richiesta più vicina all'estremo iniziale, per poi venire resettata fino alla richiesta più vicina all'estremo finale, piuttosto che essere resettata all'estremo finale stesso.

\subsubsection{Esempio}
\begin{itemize}
    \item Supponiamo di effettuare le richieste 65, 40, 18, 78 su un disco avente tracce numerate da 0 a 100 in cui la testina è inizialmente posizionata sulla traccia 30 utilizzando l'algoritmo C-SCAN:
    \begin{itemize}
        \item La testina viene mossa da 30 a 18, dunque vengono traversate 12 tracce
        \item La testina viene mossa da 18 a 0 per poter raggiungere l'estremo iniziale,
        dunque vengono traversate 18 tracce
        \item La testina viene resettata venendo spostata da 0 a 100, dunque vengono traversate 100 tracce
        \item La testina viene mossa da 100 a 78, poi da 78 a 65 e infine da 65 a 40,dunque vengono traversate 60 tracce
    \end{itemize}
    \item La distanza percorsa, ossia il numero totale di tracce traversate, corrisponde a:
    \begin{center}
        12 + 18 + 100 + 60 = 190
    \end{center}
    \item Utilizzando l'algoritmo C-SCAN ottimizzato, ossia l'algoritmo C-LOOK, si avrebbe che:
    \begin{itemize}
        \item La testina viene mossa da 30 a 18, dunque vengono traversate 12 tracce
        \item La testina viene resettata venendo spostata da 18 a 78, dunque vengono traversate 60 tracce
        \item La testina viene mossa da 78 a 65 e infine da 65 a 40, dunque vengono traversate 38 tracce 
    \end{itemize}
    \item La distanza percorsa, ossia il numero totale di tracce traversate, corrisponde a
    \begin{center}
        12 + 60 + 38 = 110
    \end{center}
\end{itemize}



\subsection{Dischi a stato solido}
I \textbf{dischi a stato solido} sono una memoria secondaria realizzata tramite memoria flash o dei chip DRAM ed una batteria a lungo termine, la quale permette a tali memorie volatili di mantenere l'informazione.\par
Pro:
\begin{itemize}
    \item Non hanno meccanismi moventi al loro interno, risultando quindi estremamente più veloci rispetto ai dischi magnetici
    \item L'accesso ai blocchi viene effettuato in modo diretto tramite un riferimento al numero di blocco, rimuovendo la necessità di uno scheduling.
    \item Le operazioni di lettura sono molto più veloci rispetto ad un normale disco magnetico
\end{itemize}
Contro:
\begin{itemize}
    \item Costo maggiore rispetto ad un disco magnetico
    \item Le operazioni di scrittura sono molto più lente rispetto ad un disco meccanico, poiché i blocchi non vengono realmente sovrascritti ma solo dereferenziati (ossia resi inaccessibili), rendendo più lento il ciclo di eliminazione dei dati
    \item Il numero di "scritture" per un blocco è limitato
\end{itemize}
\subsection{RAID}
Un \textbf{RAID (Redundant Array of Inexpensive Disks)} è costituito da un insieme di molti dischi magnetici di basso costo utilizzati come archiviazione di grandi quantità di dati.
Vengono dotati di una forma di duplicazione dei dati, detto mirroring, rendendoli ridondanti nei vari dischi, in modo da poter aumentare l'affidabilità e la velocità delle operazioni di lettura e scrittura, poiché è richiesta una minore quantità di tempo per ricercare i dati rispetto all'uso di pochi dischi magnetici di grandi dimensioni.
\pagebreak
\section{Esercitazione}
\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Quante e quali modalità di esecuzione deve 
garantire la CPU (al minimo)?]

Almeno 2: kernel e user mode
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Qual è la differenza tra kernel e user mode?]
Quando la CPU si trova in kernel mode è in grado di eseguire istruzioni privilegiate; in user mode può eseguire solamente istruzioni non-privilegiate
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=La transizione da user a kernel mode avviene:]
\begin{itemize}
    \item Quando scade il quanto di tempo assegnato al processo in esecuzione
    \item Quando un processo esegue una fork()
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Le chiamate di sistema:]
\begin{itemize}
    \item Consentono ad un programma di richiedere l'esecuzione di istruzioni privilegiate
    \item Devono essere implementate in spazio kernel
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Differenza tra:]
\begin{itemize}
    \item Chiamate di sistema iniziate via software per ottenere un servizio dal sistema operativo (ad es., I/O)
    \item Eccezioni iniziate via software per rispondere ad una situazione anomala (ad es., divisione per 0)
    \item Interruzioni iniziate da dispositivi hardware per notificare un certo evento (ad es., timer)
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Il System Call Handler:]
\begin{itemize}
    \item Utilizza una tabella dedicata che contiene ogni chiamata di sistema supportata
    \item Salva e ripristina lo stato della computazione su appositi registri
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title= Qual è la principale differenza tra programma e processo]
Un programma è un'entità statica rappresentata dal codice eseguibile ("testo"); un processo è un'entità dinamica il cui ciclo di vita è interamente gestito dal sistema operativo
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Il Process Control Block (PCB)]    
    \begin{enumerate}
        \item È la struttura dati utilizzata dal sistema operativo per rappresentare ogni singolo processo
        \item Contiene l'identificatore del processo e del relativo processo padre
        \item Contiene il valore del program counter e dello stack pointer
    \end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Descrivere cosa si intende per context switch]
È la procedura con cui un processo attualmente in esecuzione sulla CPU, viene sostituito da un altro processo "schedulabile" (nella coda dei processi pronti). Per fare ciò è necessario che il sistema operativo salvi lo stato del processo corrente (nel suo PCB) e ripristini il PCB del processo da mandare in esecuzione
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Un context switch scaturisce sempre a fronte di:]
\begin{itemize}
    \item Un segnale di interruzione da parte del timer
    \item Una chiamata fork()
\end{itemize}
\end{tcolorbox}

Indicare quale gerarchia di processi corrisponde al codice seguente:
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/qc1}}
        \caption{}
    \end{center}
\end{figure}  

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.4\textwidth,keepaspectratio]{{im/qc11}}
        \caption{}
    \end{center}
\end{figure}  

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Descrivere le controindicazioni di un'implementazione user thread "pura":]
\begin{itemize}
    \item Nessuna concorrenza/parallelismo reale
    \item Le politiche di scheduling possono essere non ottimali
    \item Occorre prevedere chiamate di sistema non bloccanti, altrimenti
    tutti i thread all'interno dello stesso processo saranno bloccati
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=I sistemi non preemptive lo scheduler della CPU interviene:]
\begin{itemize}
    \item Quando un nuovo processo viene creato o un processo esistente termina
    \item Quando un processo passa dallo stato running a waiting
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Il tempo medio di attesa di un processo:]
\begin{itemize}
    \item Dipende dal tempo di arrivo
    \item È minimo quando i processi arrivano in ordine di CPU-burst
\end{itemize}
\end{tcolorbox}


\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Calcolare il tempo medio d'attesa (average waiting time) dei seguenti processi assumendo una politica di scheduling round robin con time slice = 3 nessuna attività di I/O e context switch trascurabile]
\end{tcolorbox}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/ex}}
        \caption{}
    \end{center}
\end{figure}  
\pagebreak
\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black, title=Completare la tabella di seguito assumendo che il quanto di tempo sia pari a 2 non vi sia I/O e che tutti i processi arrivino al tempo t=0]
\end{tcolorbox}

\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/tabella}}
        \caption{}
    \end{center}
\end{figure}  
\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black, title=Lo scheduling Shortest Job First (SJF):]
\begin{itemize}
    \item Privilegia implicitamente i processi I/O-bound
    \item È un esempio di priority scheduling
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black, title= È preferibile progettare un'applicazione multi-thread anziché multi-process perché:]
\begin{itemize}
    \item I context switch tra thread sono meno onerosi
    \item È possibile ottenere parallelismo reale in caso di architetture
    multi-processor
    \item La comunicazione intra-thread è più veloce
\end{itemize}
\end{tcolorbox}


\pagebreak
\section{Esercizi \& Hack}
\begin{itemize}
    \item Il tempo di trasferimento totale per un'operazione di I/O da disco magnetico è pari a 30 msec. Sapendo che il seek time complessivo è pari a 18 msec, il rotational delay complessivo è pari a 7 msec e che il transfer rate è pari a 1.5 Gbit/sec, qual è la quantità totale di dati trasferita? \par(Si ricordi che 1 B = 1 byte = 8 bit e 1 MB =$10^{3}$ KB = $10^{6}$ B)\par
    \textbf{Svolgimento:}\par
    DTT = 30 msec\par
    Seek Time: 10 msec\par
    Rotation Delay: 7 msec\par
    Transfer Rate: 1.5 Gbit/sec\par
    \begin{equation}
        DTT = Seek Time\ + Rotation\ Delay +\ \frac{x}{Transfer\ Rate}
    \end{equation}
    quindi:\par
    \begin{center}
        30 msec = 18 msec + 7 msec + $\frac{x}{1.5 Gbit/s}$\par
        5 msec = $\frac{x}{1.5 Gbit/s}$\par
        x = 5 msec $\cdot$ 1.5 $\cdot$ $\frac{1}{8}$ Gbit/s\par
        x = 5 $\cdot$ $10^{-3}$ $\cdot$ 1.5 $\cdot$ $10^{9}$ byte/s\par
        x = 0,9375 byte/sec = 935,5 KB
    \end{center}

    \item Data una memoria fisica composta da 3 frame fisici e un processo composto da 5 pagine virtuali: A, B, C, D, E, si calcoli il numero di page fault che si verificano a fronte delle seguenti richieste da parte del processo: A, B, E, C, E, D, D, A, B. Si assuma che nessuna pagina del processo sia inizialmente caricata in memoria e che si utilizzi un algoritmo \textbf{FIFO} di sostiuzione delle pagine.\par
    \textbf{Svolgimento:}\par
    Leggiamo il numero di caratteri e inseriamo nei nostri 3 frame fisici i primi tre elmenti: A, B, E. Il nostro contatore $\alpha$ dei page fault si trova a +3. Successivamente togliamo A che sta dentro da più tempo e inseriamo il prossimo carattere (che non deve essere già presente nei nostri 3 frame). In questo caso inseriamo C. Quindi ora abbiamo C B E e incrementiamo $\alpha$ +1. Leggiamo il prossimo carattere D e lo sostituiamo a B che sta dentro da più tempo e incrementiamo $\alpha$ +1. Ora abbiamo C D E e sostituiamo C con con A e incrementiamo $\alpha$ +1, e ora abbiamo nei nostre 3 frame A D E. Infine togliamo E e leggiamo B e incrementiamo $\alpha$ +1. Il contatore dei page fault $\alpha$ = 7.
    \item Data una memoria composta da 3 frame fisici e un processo composto da 5 pagine virtuali: A, B, C, D, E, si calcoli il numero di page fault che si verificano a fronte delle seguenti richieste da parte del processo: B, C, C, B, A, E, B, A, E, D, B. Si assuma che nessuna pagina del processo sia inizialmente caricata in memoria e che si utilizzi un algoritmo \textbf{LRU} di sostituzione delle pagine.\par
    \textbf{Svolgimento:}\par
    Se la sequenza è: B C C B A E B A E D B\par
    Inizialmente abbiamo 3 page fault, derivanti dal fatto che carichi B, C e A.
    Alla richiesta di E, hai un page fault ma la pagina che LRU rimpiazza non è B, bensì C. Infatti, tra le tre pagine candidate ad essere rimpiazzate (A, B, C), A è quella che viene richiesta immediatamente prima di E, B viene richiesta subito prima di A e, infine, la terzultima è C. Quindi il nuovo stato della memoria sarà E B A e non C A E.
    Una confusione derivi dal fatto che B viene richiesta in assoluto per prima, ma quello a cui bisogna fare attenzione è che B viene richiesta anche successivamente e dopo C.\par
    \begin{center}
        \begin{itemize}
            \item B C A = 3
            \item B E A = 1
            \item D E A = 1
            \item D E B = 1
        \end{itemize}
    \end{center}
    \item Si consideri un sistema che utilizza uno scheduler randomizzato (lottery scheduler) e che suddivide i processi in A, B e C. A ciascun processo di ogni categoria vengono assegnati rispettivamente 5, 3 e 1 ticket. Assumendo che vi siano 10 processi di tipo A, 5 di tipo B e 3 di tipo C, qual è la stima della probabilità che lo scheduler assegni al processo di tipo A?\par
    \textbf{Svolgimento:}\par
    Per calcolare la probabilità che uno scheduler randomizzato (lottery scheduler) assegni un processo di tipo A, dobbiamo considerare il numero totale di biglietti nel sistema.

    Nello scenario, ci sono 10 processi di tipo A, a ciascuno dei quali sono assegnati 5 biglietti, quindi il totale di biglietti per i processi di tipo A è 10 * 5 = 50.
    
    Allo stesso modo, ci sono 5 processi di tipo B con 3 biglietti ciascuno, per un totale di 5 * 3 = 15 biglietti, e 3 processi di tipo C con 1 biglietto ciascuno, per un totale di 3 * 1 = 3 biglietti.
    
    In totale, ci sono 50 + 15 + 3 = 68 biglietti nel sistema.
    
    La probabilità che uno scheduler randomizzato assegni un processo di tipo A sarà quindi il rapporto tra il numero di biglietti assegnati ai processi di tipo A e il numero totale di biglietti nel sistema.
    \begin{center}
        Probabilità = $\frac{Biglietti di tipo A}{Totale dei biglietti nel sistema}$
    \end{center}
    In questo caso avremo = $\frac{50}{68}$ $\approx$ 0.735 (o 73.5\%)

    \item Si consideri un sistema operativo che utilizza indirizzi logici da 21 bit, indirizzo fisico da 16 bit e memoria paginata in cui ciascuna pagina ha dimensione 2 KiB (2048 bytes). Qual è la dimensione massima di memoria fisica supportata dal sistema?\par
    \textbf{Svolgimento:}\par
    \begin{center}
        $21-16 = 5$ \par
        $2^{5} = 32$\par
        $\frac{2048}{32} = 64$\par
    \end{center}

    \item Si supponga di avere una memoria M di capacità pari a 4 KiB, ossia 4,096 bytes. Assumendo che l'indirizzamento avvenga con lunghezza di parola (word size) pari 2 bytes e che M utilizzi una gestione paginata con blocchi di dimensione pari a S = 128 bytes, quanti bit sono necessari per identificare l'indice di pagina (p) e l'offset (interno alla pagina), rispettivamente?\par
    \textbf{Svolgimento:}\par
    4096 = $2^{12}$\par
    128 = $2^{7}$\par
    $p$ = 12 - 7 = 5\par
    $offset$ = 128 / 2 = 64 = $2^{6}$\par
    \item Si consideri una memoria M di capacità pari a 100 bytes con frame di dimensione pari a 10 bytes. Dato l'indirizzo del byte 37, quale sarà l'indirizzo di pagina (p) e l'offset (interno alla pagina).\par
    \textbf{Svolgimento:}\par
    $p$ = 37 / 10 = 3\par
    $offset$ = 37 - (3 $\cdot$ 10) = 7\par
    \item Si supponga che il tempo di accesso alla memoria fisica sia $t_{MA}$ = 50 nsec. e che il tempo per la gestione di un page fault $t_{FAULT}$ sia pari a 15 msec.Assumendo che la probabilità che si verifichi un page fault sia P = 0.0002, qual è il tempo complessivo atteso di accesso alla memoria?\par
    \textbf{Svolgimento:}\par
    $t_{MA}$ = 50 nsec\par
    $t_{FAULT}$ = 15 msec = 15 $\cdot$ 1000000 = 15000000 microsec\par
    p = 0.0002\par
    \begin{equation}
        t_{ACCESS} = (1 - p) \cdot t_{MA} + p \cdot t_{FAULT}
    \end{equation}
    Quindi:\par
    $t_{ACCESS}$ = (1 - 0,0002) $\cdot$ 50 ns + 0,0002 $\cdot$ (15 $\cdot$ 1000000) = 3050 ns = 3.05 microsec
    \item Si consideri un disco magnetico composto da 128 cilindri/tracce, numerati da O a 127 (O indice del cilindro/traccia più esterno/a rispetto al centro del disco), la cui testina si trova inizialmente sul cilindro 42. Si calcoli il numero di cilindri/tracce attraversate dalla testina del disco, assumendo che la sequenza di richieste: 74, 50, 32, 55, 81 venga gestita da un algoritmo di scheduling \textbf{SSTF} (Shortest Seek Time First) e trascurando il tempo di rotazione.\par
    \textbf{Svolgimento:}\par
    Sceglie la richiesta più vicina alla posizione della testina. Abbiamo le richieste: 74, 50, 32, 55, 81 con posizione attuale a 42: 
    \begin{center}
        D = 0 +  $|$ 50 - 42$|$  = 8\par
        D = 8 +  $|$ 50 - 55$|$  = 13\par
        D = 13 + $|$ 74 - 55$|$  = 32\par
        D = 32 + $|$ 81 - 74$|$  = 39\par
        D = 39 + $|$ 81 - 32$|$  = 88\par
    \end{center}
    \item Si consideri un disco magnetico composto da 100 cilindri/tracce, numerati da 0 a 99 (0 indice del cilindro/traccia più esterno/a rispetto al centro del disco), la cui testina si trova inizialmente sul cilindro 11. Si calcoli il numero di cilindri/tracce attraversate dalla testina del disco, assumendo che la sequenza di richieste: 24, 16, 77, 49, 82 venga gestita da un algoritmo di scheduling \textbf{SCAN (non-ottimizzato)}, che la testina si stia muovendo verso l'esterno (i.e., verso i cilindri con numeri più bassi) e trascurando il tempo di rotazione.\par
    \textbf{Svolgimento:}\par
    Si parte da 11 e si scende verso lo 0 e poi si risale:
    \begin{center}
        D = 0 +  $|$ 0 - 11$|$  = 11\par
        D = 11 + $|$ 16 - 0$|$  = 27\par
        D = 27 + $|$ 24 - 16$|$ = 35\par
        D = 35 + $|$ 49 - 24$|$ = 60\par
        D = 60 + $|$ 77 - 49$|$ = 88\par
        D = 88 + $|$ 82 - 77$|$ = 93\par
    \end{center}

    \item Si consideri un disco magnetico composto da 128 cilindri/tracce, numerati da 0 a 127 (O indice del cilindro/traccia pi esterno/a rispetto al centro del disco), la cui testina si trova inizialmente sul cilindro 31. Si calcoli il numero di cilindri attraversate dalla testina del disco, assumendo che la sequenza di richieste: 42, 37, 27, 104, 68 venga gestita da un algoritmo di scheduling \textbf{SCAN (ottimizzato)}, che la testina si stia muovendo verso l'esterno (i.e., verso i cilindri con numeri più bassi)trascurando il tempo di rotazione.\par
    \textbf{Svolgimento:}\par
    \begin{center}    
        D = 0 + $|$ 31 - 27 $|$   = 45\par
        D = 4 + $|$ 37 - 27 $|$  = 130 \par
        D = 14 + $|$ 42 - 37 $|$ = 276 \par
        D = 19 + $|$ 68 - 42 $|$ = 361\par
        D = 45 + $|$ 104 - 68 $|$ = 469 \par
    \end{center}
    \pagebreak
    
    \item Si consideri un disco magnetico composto da 200 cilindri/tracce, numerati da 0 a 199(0 indice del cilindro/traccia più esterno/a rispetto al centro del disco), la cui testina si trova inizialmente sul cilindro 53. Si calcoli il numero di cilindri/tracce attraversate dalla testina del disco, assumendo che la sequenza di richieste: 98,183,37, 122, 14,65,67 venga gestita da un algoritmo di scheduling \textbf{FCFS (First Come First Served)} e trascurando il tempo di rotazione.\par
    \textbf{Svolgimento:}\par
    L'algoritmo FCFS prevede che vengano analizzate le pagine in ordine 98, 183, 37, 122, 14, 65, 67 partendo da 53:\par
    \begin{center}
        D = 0 + $|$ 53 - 98 $|$   = 45\par
        D = 45 + $|$ 98 - 183 $|$  = 130 \par
        D = 130 + $|$ 183 - 37 $|$ = 276 \par
        D = 276 + $|$ 37 - 122 $|$ = 361\par
        D = 361 + $|$ 122 - 14 $|$ = 469 \par
        D = 469 + $|$ 14 - 65 $|$ = 520 \par
        D = 520 + $|$ 65 - 67 $|$  = 522 \par
    \end{center}
    \item Calcolare il tempo medio di attesa (average waiting time) dei seguenti processi, assumendo una politica di scheduling Shortest Job First preemptive (SJF). Nel calcolo, si consideri trascurabile il tempo necessario ad eseguire il
    context switch:\par
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.3\textwidth,keepaspectratio]{{im/SJF}}
            \caption{Tabella conversioni}
        \end{center}
    \end{figure}  
    \textbf{Svolgimento:}\par
    Iniziamo da A e inseriamo i sue valori fino all'inizio di B. Una volta che il processo B viene creato, il suo CPU burst stimato, ossia 4, viene comparato con il CPU burst rimanente del processo A, ossia 7. Poichè 4 $<$ 7, allora il processo B prende controllo della CPU. Analogamente, nell'istante in cui il processo C viene creato, il suo CPU burst stimato, ossia 9, viene comparato con quello rimanente del processo A, ossia 6, e quello del processo B, ossia 3. Siccome 3 $<$ 7 $<$ 9, allora il processo B mantiene il controllo della CPU. Lo stesso ragionamento viene effettuato anche dopo la creazione del processo D, dove il processo B mantiene il controllo (2 $<$ 7 $<$ 5 $<$ 9).\par
    Di conseguenza, il processo B verr`a eseguito fino al suo completamento.\par
    \begin{figure}[hbt]
        \begin{center}
            \includegraphics[width=0.3\textwidth,keepaspectratio]{{im/153322}}
            \caption{Tabella conversioni}
        \end{center}
    \end{figure}  




\pagebreak
Nota:\par
\begin{itemize}
    \item \textbf{Scan (non-ottimizzato)}: Si muove verso l'estremo più vicino a scendere verso lo 0 e poi in ordine crescente.
    \item \textbf{Look (Scan ottimizzato)}: Si muove verso l'estremo più piccolo ma senza arrivare allo 0.
    \item \textbf{C-Scan (non-ottimizzato)}\footnote{Nota che conoscere la direzione in cui si muove la testina ha senso solo per SCAN e C-SCAN.
    }: La testina si muove verso l'estremo iniziale 0 e poi da quello massimo 100 a scendere.
    \item \textbf{C-Look (ottimizzato)}: La testina si muove verso l'estremo iniziale più piccolo (0 non compreso) e poi da quello più grande a scendere 
\end{itemize}
    
\end{itemize}

\subsubsection{Un Process Scheduling Solver online}
\href{https://boonsuen.com/process-scheduling-solver}{\textit{process-scheduling-solver}}



\subsubsection{Tabella delle conversioni}
\begin{figure}[hbt]
    \begin{center}
        \includegraphics[width=0.6\textwidth,keepaspectratio]{{im/conversioni}}
        \caption{Tabella conversioni}
    \end{center}
\end{figure}  
\end{document}

